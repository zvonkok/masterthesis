
%************************************************
\chapter{Literature Review}\label{ch:literature_review}
%************************************************

The first thing to answer is why should someone do general purpose computing on
GPUs (GPGPU) anyway. For the most people\glspl{CPU}are just enough. They do not
demand on high computational power and on a high bandwidth. Still there is
paradigm shift taking place and this can not be neglected. As stated in
\citep{citeulike:1187394} and in \citep{citeulike:3421647} CPU manufacturers are
facing problems which they cannnot overcome just by increasing the frequency.
The often cited \emph{Walls} are first, \emph{The Memory
Wall}\citep{citeulike:457955}, \emph{The Frequency Wall} and \emph{The Power
Wall}. The only way seen by CPU makers is currently to go multicore. Intel e.g.
went multicore with there new \emph{Core Microarchitecture} for consumer
products and even a \gls{GPU} replacement, \emph{Larrabee} \citep{citeulike:3153758}.
IBM, Toshiba and Sony developed the \emph{Cell Broadband Architecture} a 9 core
chip \citep{citeulike:1243173}. SUN developed the \emph{Niagara} CPU a multi-core
general purpose processor. It has eight in-order cores, each of them capable of
executing four simultaneous threads \citep{citeulike:3743958}.

Compared to\glspl{CPU}GPUs went years ago to multicore and multithreading.\glspl{GPU}are
maybe the kind of processor where\glspl{CPU}are heading to in terms of multithreading
and raw performance. Forecasts project that every two years the amount of cores
can double. The multicore approach may be the answer to the problems stated
above but this yields to another thing the \emph{parallel programming problem}
\citep{citeulike:3750573}. User will only benefit from this growth if software
can make use of all the cores. Many developers learned about the
single-threaded von neumann model and are not familiar with parallel code which
is subject to errors such as deadlocks and livelock, race conditions and many
more. Parallel programming is difficult and there are several paradigms to make
life easier for developers. 

On of these is the data-parallel paradigm. Where one
is not trying to assign diffferent subtasks to separate cores rather assigning
an individual data element to a separate core for processing
\citep{citeulike:3750565}. 3D rendering, an embarassingly data-parallel problem,
has driven the \gls{GPU} evolution which makes the \gls{GPU} a perfect target for
data-parallel code. There are several fine-grained or data-parallel programming
environments that leverage the \gls{GPU} for general purpose computing (Brook, Sh,
RapidMind,\ldots). 

The focus of this work will be CUDA\footnote{www.nvidia.com}. \gls{CUDA}is the only
environment which is not based on a graphics library and officialy released by
{\slcsmallcaps{NVIDIA}} for their GPUs. \gls{CUDA}is a minimal extension to C and C++ programming
languages. The technique employed by \gls{CUDA}is single process, multiple data
 (SPMD). Tasks are split up and run simultaneously on multiple threads (cores)
with different input \citep{citeulike:3072519}.

\section{Embarrassingly Parallel Algorithms} % (fold)
\label{ssub:choosing_a_fast_algorithm}
Before even digging into the wide field of algorithms and difficult problems in
high performance computing one has to understand the hardware architecture of
the\glspl{GPU}to make a decision if an algorithm can be mapped on GPUs. A pretty good
overview over the {\slcsmallcaps{NVIDIA}} \gls{GPU} gives the \emph{{\slcsmallcaps{NVIDIA}} \gls{CUDA}Programming Guide}
\citep{citeulike:3325943}. A little bit outdatet but still of interest is
\emph{The GeForce 6 Series \gls{GPU} Architecture}\citep{citeulike:3757915} which gives
an overview how the \gls{GPU} fits into the whole system, what a fragment processor,
vertex processor or what textures are. To have a even deeper look into\glspl{GPU}the
article \citep{citeulike:2790995} is highly recommended.

\subsection{Computations which Map Well to GPUs} % (fold)
\label{par:computations_which_map_well_to_GPUs}
It is important to understand that\glspl{GPU}are good at running computer graphics
and algorithms which \emph{mimic} or have the attributes of computer graphics in
terms of data parallelism and data independence. Not only that similar
computations are applied to streams of many data elements (vertices, fragments,
...) but also the computation of each element is completely or almost completely
independent \citep{citeulike:3733428}. Such types of algorithms are often called
embarassingly parallel algorithms where subtasks rarely or never communicate to
each other.

Another important fact for a algorithm is the \emph{Arithmetic Intensity}. The
Arithmetic Intensity is the ratio of computation to bandwidth or formally:
\begin{center} 
 \emph{arithmetic intensity = operations / words transferred.}
\end{center}
This fact is important because the increase of computational throughput is
faster than the memory throughput which leads to the problem known as \emph{The
Memory Wall}. \gls{GPU} memory systems are architected to deliver high bandwidth,
rather than low-latency, data access. As such computations that benefit most of
the\glspl{GPU}have a high arithmetic intensity \citep{citeulike:3733428}. The next 
sections will represent some algorithms which could fit to GPUs. 
% paragraph computations_which_map_well_to_GPUs (end)

\subsection{Ray Tracing} % (fold)
\label{par:ray_tracing}
Ray Tracing \citep{citeulike:841961} is an embarrassingly parallel alorithm which
could fit well to GPUs. The author has a extensible knowledge of Ray Tracing on
massively parallel computers \citep{citeulike:80546}. Thats why Ray Tracing was
first considered for porting to the GPU. Unfortuneately there were several
implementations already done for the GPU. Nevertheless equipped with all the
knowledge about Ray Tracing and how to split up the work, arrange the data on a
parallel machine to run efficiently the Ray Tracing algorithm
\citep{citeulike:3770900} will be used for initial benchmarks and the feasibility
study \autoref{chap:feas}.
% paragraph ray_tracing (end)

\subsection{Photon Mapping} % (fold)
\label{par:photon_mapping}
Ray Tracing has a local illumination model. To generate more realistic effects
like caustics, diffuse / glossy indirect illumination and more a more
sophisticated model has to be used. Global illumination like \emph{Photon
Mapping} \citep{citeulike:635695} can create all the effects that Ray Tracing
cannot. There are implementations of photon mapping on GPUs
\citep{Purcell:2003:PMO} which are developed with graphics apis and not with
CUDA. Anyhow since photon mapping is heavily using a kd-tree it would be a major
effort to develop an efficient data structure which has the same functionality
as a kd-tree. Nevertheless the first candidate for porting to the \gls{GPU} is \emph{Photon Mapping}.
% paragraph photon_mapping (end)

\subsection{Multiple Precision Arithmetic} % (fold)
\label{par:multiple_precision_arithmetic}
Another interesting field which demands high computational power is number
theory. The most common/known application is asymmetric cryptography. To
decipher messages that are cyphered with an asymmetric algorithm one needs
superior computational power. An overview over common algorithms gives
\citep{citeulike:3783254}. All algorithms have one thing in common they need a
multiple precision library to represent numbers with 200 and more digits. A good
overview gives \citep{citeulike:3783244}.

The idea was to implement some of the factoring algorithms to the GPU. The only
thing needed is the multiple precision library. In \citep{citeulike:3783254} the
Gnu Multiple Precision (GMP) library was used to implement the algorithms. So
the multiple precision arithmetic is another candidate for porting to the GPU.
% paragraph multiple_precision_arithmetic (end)
\subsection{High Dynamic Range} % (fold)
\label{par:high_dynamic_range}
Image processing is always a candidate for embarassingly parallel algorithms. A
pretty new algorithm to enhance images is \emph{tone mapping}
\citep{citeulike:3783303}. Tone mapping is the compression of dynamics in High
Dynamic Range (HDR) pictures. There are several algorithms for tone mapping:
Mantiuk \citep{citeulike:3783315}, Reinhard \citep{citeulike:3783311}, Durand
\citep{citeulike:789299}, Fattal \citep{citeulike:3783313} and many more. All of
this algorithms are present in the pfs-tools library written by Krawczyk
\citep{citeulike:3783303}. As this document was written Krawczyk was already
implementing the pfs-tools on \gls{GPU} but not publishing it. Furthermore there is an
complete editor written for hdr image processing which is running on the GPU. So
HDR was discarded but there are several other algorithms in image processing
which are considered as candidates. Some algorithms in no particular order:
segmentation, tracking, filtering, ... and so on. This is put as another
candidate to the list of the possible algorithms for porting. 
%paragraphhigh_dynamic_range (end)

\subsection{Genetic Algorithms} % (fold)
\label{par:genetic_algorithms}
Parallel genetic algorithms are usually implemented on parallel machines but
fine-grained parallel genetic algorithms can be mapped to GPUs
\citep{citeulike:3801879}. In \citep{citeulike:3801866} its shown what kind of
genetic algorithm map well to\glspl{GPU}and how the work and communication is
handled. It is \emph{pretty} easy to implement simple genetic algorithms on the
GPUs but with increasing complexity one has to consider many more things: load
balancing, communication pattern, dynamic memory allocation, resolving of
recursion ... . Another paper \citep{citeulike:3801883} compares genetic
algorithms implemented on\glspl{CPU}and\glspl{GPU}and shows that the latter is much more
effective than the former. All genetic algorithms have one thing in common the
core algorithm. Once effectively implemented on the \gls{GPU} the core algorithm is
extended with definitions like the population, selection, recombination and
mutation to solve a specific problem. The skill here is to choose the right
definitions and not more the efficient implementation of the core algorithm. So
this topic excluded from the candidate list.
% paragraph genetic_algorithms (end)

\subsection{Chaos Theory} % (fold)
\label{par:chaos_theory}
Another interesting field is chaos theory. Especially the visualization of
chaos. The maybe most famous visualization of chaos is \emph{The Fractal Flames
Algorithm}. Fractal flames are a member of iterated function system class of
fractals created by Scott Draves \citep{citeulike:3801950}. He uses a rather
complicated set of functions in the system to generate stunning visualization of
the iterative process of the system. The next release will have support for the
GPU. Thats why it was firstly discarded but the research about chaos led to
other interesting papers respectively books.

There is no need to use approx. 21 function for a system to generate visually
appealing pictures of chaos. Sprott showed in \citep{citeulike:3745535} that even
with very simple functions one can create patterns in chaos. This patterns are
called \emph{Strange Attractors} and are the visualization of chaotic behaviour.
There are created by iterating a simple equation some million times. 

Pickover shows in his book \citep{citeulike:3812233} how to create patterns from
a variety of sources. He shows how to create nice looking patterns from fourier
analysis, acoustic, chemistry and many more. Anyhow all of these equations or
differential systems have on thing in common no matter how complicated the
system is the core algorithm is to iterate a specific equation with correct
input numbers to create chaos. The algorithm is heavily computational bound
which makes it a good candidate for porting to the GPU.


\subsection{Image Processing}
Many image processing algorithms are embarrassingly parallel as one can calculate
filters on idividual pixels without considering the neigbouring pixels. 


\section{OLD STUFF}

%The related work section (sometimes called literature review) is just that, a
%review of work related to the problem you are attempting to solve. It should
%identify and evaluate past approaches to the problem. It should also identify
%similar solutions to yours that have been applied to other problems not
%necessarily directly related to the one your solving. Reviewing the successes or
%limitations of your proposed solution in other contexts provides important
%understanding that should result in avoiding past mistakes, taking advantage of
%previous successes, and most importantly, potentially improving your solution or
%the technique in general when applied in your context and others. In addition to
%the obvious purpose indicated, the related work section also can serve to:

%*  justify that the problem exists by example and argument,
%* motivate interest in your work by demonstrating relevance and importance,
%* identify the important issues,
%* and provide background to your solution.

%Any remaining doubts over the existence, justification, motivation, or
%relevance of your thesis topic or problem at the end of the introduction should
%be gone by the end of related work section.

%Note that a literature review is just that, a review. It is not a list of
%papers and a description of their contents! A literature review should critique,
%categorize, evaluate, and summarize work related to your thesis. Related work is
%also not a brain dump of everything you know in the field. You are not writing a
%textbook; only include information directly related to your topic, problem, or
%solution.


\subsection{An Overview of Stream Computation} % (fold)
\label{sub:an_overview_of_stream_computation}
streaming programming model... sdk's .. 
% subsection an_overview_of_stream_computation (end)

\subsection{General Programming of Streaming Processors} % (fold)
\label{sub:programming_streaming_processors}
SDK's, CTM, Anbieter, verschieden Programiersprachen
% subsection programming_streaming_processors (end)

\subsection{Stream Computing in Field Use} % (fold)
\label{sub:stream_computing_in_use}
Einsetzbarkeit, Leistung und Effizienz von \gls{GPU}Applikationen im nichtgrafischen Applikationsbreich Kontext, Referenzen (wo wird eingesetzt im nicht-grafik
Bereich)
% subsection stream_computing_in_use (end)

\subsection{NVIDIA CUDA} % (fold)
\label{sub:nvidia_cuda}
Tesla, Stil, Aufwand, Debugbarkeit, Portieraufwand für 
General Purpose Anwendungen, Vergleich zu SPUFS
% subsection nvidia_cuda (end)

\subsection{The Classic GPU Pipeline} % (fold)
\label{sub:the_classic_gpu_pipeline}
% subsection the_classic_gpu_pipeline (end)

\subsection{The GeForce 8 Series Architecture} % (fold)
\label{sub:the_geforce_8_series_architecture}
% subsection the_geforce_8_series_architecture (end)

% section related_work_and_background (end)

\section{Algorithmic point of view to GPUs} % (fold)
\label{sec:algorithmic_view_to_gpus}
Welche Algorithmen eignen sich fuer GPUs
\subsubsection{Computational Concepts} % (fold)
\label{ssub:computational_concepts}
% subsubsection computational_concepts (end)

\subsubsection{Efficient Data Structures} % (fold)
\label{ssub:efficient_data_structures}
% subsubsection efficient_data_structures (end)

\subsubsection{Program Optimization} % (fold)
\label{ssub:program_optimization}
% subsubsection program_optimization (end)

% section algorithmic_view_to_gpus (end)
%\lstinputlisting[title=A Curriculum Vit\ae]%
 %   {Examples/classicthesis-cv.tex}
    \graffito{\dots or your supervisor might use the margins for some
    comments of her own while reading.}

%*****************************************	





%*****************************************
\chapter{Examples}\label{ch:examples}
%*****************************************
\section{Proposed Solution} % (fold)
\label{sec:proposed_solution}
%At this point the reader will have enough background (from the related work and introduction) to begin a detailed problem analysis and solution proposal. You should clearly identify in detail what the problem is, what you believe are the important issues, describe your proposed solution to the problem, and demonstrate why you believe your particular proposal is worth exploring. Note you might have one or more variants that are worth exploring. This is okay assuming you have time to explore them as they can be compared experimentally if you cannot clearly justify the preference for a particular varient.

%You must also clearly identify what the outstanding issues are with your solution. These are the issues that must be resolved by experiment. If you don't need to experiment, you must have proved your solution correct. This situation is occurs in mathematics, but it is rare in operating systems.
% section proposed_solution (end)


\section{Experimental Results} % (fold)
\label{sec:experimental_results}
%The reader now knows your proposed solution(s), understands the problem in detail, and knows what are the outstanding issues. You can now introduce the experiments you used to resolve the outstanding issues in your solution. You must describe how these experiments resolve the outstanding issues. Experiments without clear motivation why they were conducted are a waste of paper, give me an interesting novel to read if you really feel compelled to give me dead trees.

%Describe the experimental set up in such a way that somebody could reproduce your results. This should be aimed at the level of somebody externally tackling the same problem, using your solution, and wanting to verify your results. This should not be targeted at the level of somebody within the local group, using your code, on our machines. Details such as  "do blah on machine X to get machine Y to perform monitor" should not be in a thesis. Such information is useful, but make it available outside your thesis.

%Present the results in a comprehendible manner. Describe them in words. Don't simply include ten pages of tables and graphs. Again, buy me a book instead. Make sure that the tables and graphs have clear labels, scales, keys, and captions.
Zugriffsarten, Profiling, Latencies, Bandbreiten, Berechnungen
Is the DMA engine determnisitic?
% section experimental_results (end)


\section{... on the GPU} % (fold)
\label{sec:_on_the_gpu}
\subsubsection{Analysis} % (fold)
%This section takes the outstanding issues you previously identified, the experimental results, and analyzes them. Did the experimental results substantiate your solution, and how do they substantiate your solution. Where the results what you expected? Did the experiments create new issues? If so, identify them.

%By the end of this section the reader should know how your proposed solution worked out. The reader should know what issues were resolve, what the resolution was, and what issues remain.


\colorbox{red}{10.2 Reducing Cost of Fitness with Caches}

\label{ssub:analysis}
% subsubsection analysis (end)

\subsubsection{Design} % (fold)
\label{ssub:design}
% subsubsection design (end)

% section _on_the_gpu (end)

\section{Methodology and Discussion} % (fold)
\label{sec:appraisal_of_achievement}
%Discuss and explain your results. Show how they support your thesis (or, if they don't, come up with a damned good reason real quick). It is important to separate objective facts clearly from their discussion (which is bound to contain subjective opinion). If the reader doesn't understand your results, you probably do neither. And this will be reflected in the assessment.
Methods applied, Results achieved if not why, 
Benchmarks, it would be good if results are discussed in first placed
and then discrepancies here discussed.
% section appraisal_of_achievement (end)

\section{Conclusion, Questions, Perspective} % (fold)
\label{sec:conclusion_questions_perspective}
%Recap on your thesis. It has been a long journey if the reader has made it this far. Remind the reader what the big picture was. Briefly outline your thesis, motivation, problem, and proposed solution.

%Now the most important part, draw conclusions based on your analysis. Did your proposed solution work? What are the strong points? What are the limitations?

%Significant issues identified in the thesis, or still outstanding after the thesis, should be describe as future work.

% section conclusion_questions_perspective (end)

\section{Summary} % (fold)
\label{sec:summary}
% section summary (end)
bal bla blabub
%Examples: \textit{Italics}, \spacedallcaps{All Caps}, \textsc{Small
%Caps}, \spacedlowsmallcaps{Low Small Caps}.
%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
