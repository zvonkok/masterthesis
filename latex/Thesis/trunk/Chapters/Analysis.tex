%!TEX TS-program = pdflatex
%!TEX encoding = UTF-8 Unicode
%!TEX TS-options = -halt-on-error
%!TEX TS-options = -shell-escape
\chapter{Mean Shift Algorithm Analysis} % (fold)
\label{ch:algorithm_analysis}
Before any porting can be started the developer has to find out if there are
multiple activities or tasks which can run simultaneously which expose
exploitable concurrency. The developer has to find concurrency either by
decomposing the data or tasks. By this decomposition one wants to solve bigger
problems in less time as several processing units can solve different parts of
the problem.

But before any analysis is started one has to know if the problem is large
enough and if the resulting speedup justifies all the effort that is expended on
making a parallel version out of it. In case of mean shift which is used for
several things like filtering, segmentation, pattern recognition and real time
tracking one can deduce that for bigger images or many images the computation
time climbs fast as the dsize or the number of images rises. To have a clue how
the mean shift algorithm behaves with big pictures several run times where
recorded. For the analysis of the mean shift algorithm a ready to use
\gls{EDISON} was used that was profiled and modified and parallelized for the
purpose of this thesis.

The \gls{EDISON}
\footnote{http://www.caip.rutgers.edu/riul/research/code/EDISON/index.html}
system which was developed by the authors
\citeauthor{citeulike:462300} of \citep{citeulike:462300} offers
functionality to filter, segment and detect edges in images.
 
The \autoref{fig:cpu_run_time} shows results of \gls{CPU} run times of
the \gls{EDISON} application dependant on image size. The vertical
axis shows the run time in seconds and the horizontal axis shows the
side length in pixels of the quadratic \autoref{fig:orig0}.
\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      % smooth,
      % stack plots=y,
     % area style,
      ybar, bar width=8pt,
      width=\textwidth,
      height=5cm,
      xtick={128,384,...,2688},
      axis x line=bottom,
      axis y line=left,
      xmin=128, xmax=2816, 
      % ymin=0, 
			ymax=7000,
      xlabel=Image size, ylabel={Seconds [s]}, 
			%enlargelimits=0.03,
      %ymajorgrids
 			]
      \addplot[color=pl0,fill=pf0]
      table[x=PIX] {Plots/single.gpu.data}%
      \closedcycle;
    \end{axis}
  \end{tikzpicture}%
  \caption{CPU run time depending on the image size}\label{fig:cpu_run_time}%
\end{figure}
Considering the numbers in the result one can see that the run times
grow linear to the image sizes. The mean shift algorithm has linear
complexity, hence its complexity can be written as $O(n)$. Doubling
the side length of the quadratic picture the run time increase by a
factor of 4. See e.g.  the run times for side length: (l=256, t=9) and
(l=512, t=36).
    
But this is obvious, as stated in \autoref{ch:mean_shift} for each
pixel of the image the mean shift vector has to be calculated, and if
one increases the number of pixels by a number $n$ we have to deal
with $n$ times longer run time. Such consideration have to be done in
the run-up to have a clue to which point the algorithm can be
accelerated. The problem has to be well understood.

The next important step is to find parts which can be offloaded to an
accelerator as the \gls{GPU}. A first good point for such parts is to
make a profiling of the application to find out the most
computationally intensive parts. It makes no sense to parallelize
functions which contribute only 1\% to the run time.
    
\section{Profiling the Original Code} % (fold)
\label{sec:run_time_analysis_of_the_original_code}
A good start point is to take a profiler und generate a profile of the
functions, recording their run time and call history. In this case a
statistical profiler is used which operates by sampling. The samples
are taken from the hardware performance counters which every modern
\gls{CPU} has builtin.
OProfile\footnote{http://oprofile.sourceforge.net/news/} a system wide
profiler for Linux was used to examine the run time of the
\gls{EDISON} program. The \autoref{tab:comped} shows the run time
analysis of \gls{EDISON}.

\begin{table}[ht]
  \centering
  \pgfkeys{/pgf/number format/.cd,fixed zerofill,precision=2}
  \pgfplotstabletypeset[
  % column type=r,
  every row no 5/.style={
    before row={\rowcolor{pf0}}
  },
  columns/Self/.style={column type=r, column name={Self (\%)}},
  columns/Total/.style={column type=r, column name={Total (\%)}},
  columns/Symbol/.style={string type, column type=l, column name={Symbol}}
  ]{Tables/run.time.edison.data}
  \caption[EDISON run time profile]{ \gls{EDISON} run time analysis}
  \label{tab:comped}
\end{table}

Before taking a closer look at the table, the columns have to be
explained. The column \emph{Total} shows how long the function plus
their child function were executing. Whereas the column \emph{Self}
shows how long the function spent executing itself without the
execution time of their child functions. The column \emph{Symbol}
shows the function that was executed.

Now having a look at the table there is one function which is
executing 98.2\% of the run time, the function
\emph{uniformLSearch(double*, double*)}. In this function the
algorithm tries to find feature points that fall into the search
window with radius $h$ (see \autoref{sec:mean_shift}). This is the
starting point as it is the most computationally intensive and the
focus for parallelization. There is a way to calculate to which extent
the particular function can be parallelized and which speedup one can
expect. Speedup is the ratio of run time when executing a program on a
single processr to the run time when using $n$ processors. Given $T_1$
the run time of a program on a single processeor and $T_n$ the run
time of the same program on $n$ processors then
\begin{equation}\label{eq:speedup}
  S(n) = \frac{T_1}{T_n}
\end{equation}
is a measure for the speedup. One familiar law to calculate the how
much speedup can be obtained through paralellism is Amdahl's law.

\section{Amdahl's law}
\label{sec:amdahl_s_law}
Amdahl's law says supposing that 80\% of a computation can be
parallelized and 20\% can't, then even if the 80\% that is parallel
were run on an infinite number of processors the highest speedup that
one can achieve is 5. Generally speaking if a fraction $p$ of a
computation can be run in parallel and the rest must run serially,
Amdahl's law upper bounds the speedup by $1/(1-p)$.

The speedup of a application according to Amdahl is given by
\begin{equation}\label{eq:amdahl}
  S_{max}(n) = \frac{1}{(1-p) + \frac{p}{n}}
\end{equation} 
where $1$ is the execution unit time of the old computation, $(1-p)$
the inherently serial part and $p/n$ the parallel part divided by $n$
the number of processing units. When $n \rightarrow \infty$ then the
execution time approaches the time for executing the sequential
program fraction. So no matter how many processors one adds to the
system it will at least execute as long as the sequential program
fraction, which is an upper bound of speedup. A important addition is
that the sequential program fraction or serial processing percentage
is relative to the overall execution time using a single processor. It
is independent of the number $n$ of processors
\citeauthor{citeulike:3838998} \citep{citeulike:3838998}.

Assuming that the mean shift filtering can be parallelized with any number of
processing units one can calculate the maximum speedup achieveable according to
Amdahl. Taking the parallel processing percentage from \autoref{tab:comp} for
the filtering step:
\begin{equation*}\label{eq:parallel}
  p = 99,3\% = 0.993 
  p\end{equation*}
we get

\begin{equation*}\label{eq:am0}
  S_{max}(n) = \frac{1}{(1-0.993)} = \frac{1}{0.007} = 142.857	
\end{equation*}

Applying the calculation with $n = 128$, the used \gls{GPU} has 128
processors one can acheive a speedup of:
\begin{equation*}\label{eq:g92sp}
  S_{max}(128) = \frac{1}{(1-0.993) + \frac{0.993}{128}} = \frac{1}{0.007 + 0.0077578125} = 67.76
\end{equation*}
One can see that even for very small serial processing percentage the speedup is
not very high. Therefore \citeauthor{citeulike:3732921} proposed a alternative
formulation where the fractions are now dependent on $n$. He assumed that for
larger problem, the fraction of a program to parallelization increases. Thats
why Gustafson's law is often referred as a scaled speedup measure and Amdahl's
as a non scaled speedup measure. Given the serial $s'$ and paralllel time $p'$ a
single processor would require $s' + p' \times n$ time to finish the execution.
The speedup is then given by:
\begin{equation}\label{eq:gus}
  S_{max}(n) = \frac{s' + p' \times n}{s' + p'} = n + ( 1 - n ) \times s'
\end{equation}
Applying the calculation with $n = 128$, the used \gls{GPU} has 128 processors
one can acheive a speedup according to Gustafson of:
\begin{equation*}\label{eq:g92sp}
  S_{max}(128) = 128 + (1 - 128) \times 0.007 = 127.11
\end{equation*}

It looks like that Amdahl's \& Gustafson's law are in contradiction but looking
more precise on both definitions one can see that both laws employ different
definitions for the fraction of the serial and parallel exectuion times. Amdahl
uses non scaled percentage and Gustafson scaled percentage. Mathematicaly they
are equal just two different formulations. The scaled percentage can be
transformed to a non scaled perecentage where Gustafson's law gives the same
results as Amdahl's law \citep{citeulike:3838998}.

In summary one can expect more than a 100 fold speedup when parallelizing the
filter step of the mean shift algorithm. The next steps will focus on analyzing
how the mean shift filter can be decomposed to take advantage of multiple
processors.

\section{Data {\itshape{\&}} Task Parallelism} % (fold)
\label{sec:data_and_task_parallelism}
There are two ways to decompose an algorithm to take advantage of parallelism.
The first way is to decompose the algorithm into several tasks that can run
independently. Each task is performing independently different calculation on
the data. This characteristic is known as task parallelism. To know if two task
can run independent one can take \citeauthor{citeulike:6113408} conditions and
evaluate them.

Let $P_i$ and $P_j$ be two tasks of a program $P$. For $P_i$ let $I_i$ be the
input and $O_i$ the output data and for $P_j$ let $I_j$ be the input and $O_j$
be the output data, then $P_i$ and $P_j$ are independent if they satisfy the
following conditions
\citep{citeulike:3838998}:
\begin{subequations}\label{eq:bern_cond}
  \begin{align}
    I_j \cap O_i & = \emptyset \label{eq:bern0}\\
    I_i \cap O_j & = \emptyset \label{eq:bern1}\\
    O_i \cap O_j & = \emptyset \label{eq:bern2}
  \end{align}
\end{subequations}
The first two formulations \autoref{eq:bern0} and \autoref{eq:bern1} are stating
that the input of one task has to be independent from the other task. Otherwise
one task would have to wait for the output of the other task to continue.
Furthermore not only the inputs vs. the outputs have to be independent but also
the outputs of each task have to be independent. If the above conditions cannot
be applied to a program $P$ than it is likewise that race conditions and poor
performance can arise through e.g. excessive synchronization overhead.


\subsection{Task Parallelism} % (fold)
\label{sub:task_parallelism}

Equipped with the knowledge how to identify independent parallel tasks, the mean
shift segmentation algorithm \autoref{alg:mss} can now be examined. Having a
look at the algorithm steps one can easily see that every task is violating
every condition stated above. The filtering step input data is dependent on the
\gls{RGB} to \gls{LUV} conversion output data. The segmentation step input data
is dependent on the filtering output data. So it does not matter how big an
image is or if it is a color or grayscale image the mean shift segmentation
algorithm will always perform all from each other dependent calculations as
described in \autoref{alg:mss}. The longest path of dependent calculations is
the critical path. For the mean shift algorithm there exists no shorter path.

In summary one can discard the idea of task parallelism for the mean shift
segmentation algorithm. Thats why the focus of parallelization is now on data
parallelism. 
% subsection task_parallelism (end)


\subsection{Data Parallelism} % (fold)
\label{sub:data_parallelism}
The second way of decomposing an algorithm is data decomposition. The data is
decomposed into chunks on which similar operations are being applied in such a
way that the different chunks can be operated on concurrently. The focus in data
parallelism is on the data structures which define the problem and not at the
tasks.

Focusing on data and having a look at the most computational intensive part (the
filtering step) one can see that each pixel of an image can be calculated
independently. The input and output pixels are independent and furthermore it
doesn't matter at which pixel one starts, the filtering step is deterministic,
starting from the same input will lead to the same output.

Another important aspect is how often the subtasks (where each subtask is
calculating one pixel) have to synchronize or communicate. An algorithm exhibits
fine-grained parallelism if the subtasks have to communicate many times,
coarse-grained parallelism if the subtasks do not communicate many times. In
this case the algorithm even exhibits embarrassingly parallel parallelism. The
subtasks never have to communicate or synchronize. Each subtask takes one or a
chunk of pixels applies the filtering steps and writes back the result. The
intermediate steps where the mean shift vector is moved over the spatial space
are also independent for each pixel.

The approach in parallelizing the mean shift algorithm is to use a data
decomposition where each subtask is filtering a chunk or a pixel of the entire
image. Depending on the number of processing units one can choose an appropriate
chunk size to exploit every unit.

If the algorithm would only exhibit fine-grained parallelism the next steps
would be to identify groups to simplify the job of managing dependencies and
have a look at the ordering to satisfy constraints among tasks. Luckily here one
has only to deal with a embarrassingly parallel algorithm and can move to the
next step, identify how data is accessed and shared among subtasks.
% subsection data_parallelism (end)
% section data_and_task_parallelism (end)

\section{Data Flow} % (fold)
\label{sec:data_flow}

It is important to understand that inefficient data access can lead to very poor
performance on every processing unit especially on a \gls{GPU}. Inefficient data
access leads to an algorithm that is memory bound which means it doesn't matter
how much computational power the processing unit has it will be bound to the
memory performance (See \autoref{ch:literature_review} for \emph{arithmetic
intensity}). Therefore its crucial to understand the data and optimize it for
access. If done incorrectly tasks may get invalid data or it could lead to
excessive synchronization overhead.

In \autoref{sub:task_parallelism} it was shown that the filtering step is
dependent on the color conversion step where the output data of the color
conversion step becomes the input data of the filtering step. Since this is done
sequentially there is no need to take care of some synchronization. This scheme
is the same for the filtering and segmentation step. Since the filtering step is
done in parallel and the segmentation needs the output data form the filtering
step a barrier after the filtering step is needed.

Now to the interesting part the parallel filtering step. For the following
considerations the viewpoint will be a single processing unit. It is firstly
enough to consider only one unit and one pixel at a time since the \gls{GPU}
exhibits a \gls{SPMD} programming model (see
\autoref{ssub:single_program_multiple_data_spmd}) and secondly the mean shift
algorithm is embarrassingly parallel. It is later easy to broaden the
examination to a cluster of processing units and chunks of pixels if its needed.

A processing unit firstly reads the corresponding pixel $x_n = (x_n^r, x_n^f)$
from the image. Then it moves the mean shift vector to the next pixel on the
path to the convergence point $z_n = (x_n^r, y_{conv}^f)$ by finding the feature
vector which falls into the search window. This new pixel is the basis for the
next iteration---calculation of the mean shift vector. On average of about
15--20 iterations the mean shift vector converges to the mean at the convergence
point \citep{DBLP:conf/eccv/ZhangKT06}. The whole path where the mean shift
vector is moving on is unpredictable. The vector can move over the whole image
or a few pixels, this is an important fact to consider when mapping the
algorithm to the \gls{GPU} because random access to the memory can decrease the
memory performance significantly. At last the convergence point is saved to the
$z_n = (z_n^r, z_n^f)$ data structure.

The segmentation step as stated above needs the data for the filtering step. The
only parallel portion of the algorithm is the filtering step so no further 
considerations will be made. 
% section data_flow (end)

\section{Summary} % (fold)
\label{sec:analysis_summary}
So far the problem has been extensively analyzed. The most compute intensive
parts have been identified and a perliminary speedup has been calculated
according to Gustafon's law. Furthermore the problem has been decomposed into
tasks that can execute concurrently using task respectively the data
decomposition. Additionally to the decomposition the dependencies between
processing units has been identified. 

The items  mentioned above will guide the design of the algorithm in the next
phase. Therefore it is important to have clearly understood the algorithm and
have done the decomposition in the right way. 

The decomposition has to be suitable for the target platform. Since the
\gls{GPU} is a \gls{SPMD} architecture, which means one program the mean shift
algorithm is operating on many data the pixels. Above it was shown that the way
of data decomposition is preferrable because (1) the pixels can be mapped 1:1 to
a processing unit on the \gls{GPU} and (2) there are enough pixel to keep the
\gls{GPU} busy. Furthermore the \gls{GPU} is a number cruncher and algorithms
doing heavy math or calculations can profit from the sheer computational power.

The decomposition has to be flexible, efficent and simple. A flexible setup
means that the amount of independent task can vary and the configuration how
they are arranged is as well selectable. This means it can be adapted to from
one to many processing units, the load can evely be distributed among the
processing units which leads to good efficiency and scalability and the
decomposition can be parameterized in terms of units and problem size.
Additionally weak dependency in the decomposition makes load balancing easier.
Simple in this case means minimizing the synchronization and communication overhead
through the decomposition, which is in this case non-existant. 

This chapter exposed the concurrency in the mean shift algorithm that can now be
taken into the next phase, designing the algorithm. 
% section summary (end)

