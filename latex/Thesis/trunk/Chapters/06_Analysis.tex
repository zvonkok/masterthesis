%!TEX TS-program = pdflatex
%!TEX encoding = UTF-8 Unicode
%!TEX TS-options = -halt-on-error
%!TEX TS-options = -shell-escape
\chapter{Mean Shift Algorithm Analysis \textit{\&} Design} % (fold)
\label{ch:algorithm_analysis}
Before any porting can be started the developer has to find out if there are
multiple activities or tasks which can run simultaneously which expose
exploitable concurrency. The developer has to find concurrency either by
decomposing the data or tasks. By this decomposition one wants to solve bigger
problems in less time as several processing units can solve different parts of
the problem.

But before any analysis is started one has to know if the problem is large
enough and if the resulting speedup justifies all the effort that is expended on
making a parallel version out of it. In case of mean shift which is used for
several things like filtering, segmentation, pattern recognition and real time
tracking one can deduce that for bigger images or many images the computation
time climbs fast as the size or the number of images rises. To have a clue how
the mean shift algorithm behaves with big pictures several run times where
recorded. For the analysis of the mean shift algorithm a ready to use
\gls{EDISON} was used that was profiled and modified and parallelized for the
purpose of this thesis.

The \gls{EDISON}
\footnote{http://www.caip.rutgers.edu/riul/research/code/EDISON/index.html}
system which was developed by the authors
\citeauthor{citeulike:462300} of \citep{citeulike:462300} offers
functionality to filter, segment and detect edges in images.
 
The \autoref{fig:cpu_run_time} shows results of \gls{CPU} run times of
the \gls{EDISON} application dependant on image size. The vertical
axis shows the run time in seconds and the horizontal axis shows the
side length in pixels of the quadratic \autoref{fig:orig0}.
\begin{figure}[ht]
 \centering
 \begin{tikzpicture}
   	\begin{axis}[
   	scaled x ticks={real:1048576},
   	xtick scale label code/.code={$\cdot 2^{20}$},
   	smooth,
    width=\textwidth,
    height=5cm,
    xtick={0, 1048576,2097152,3145728,4194304,5242880,6291456,7340032,8388608},
    axis x line=bottom,
    axis y line=left,
    xmin=64, xmax=7800000,
    % ymin=0,
    ymax=7500,
    xlabel={Image-size in [pixel]}, 
		ylabel={Time in [s]},
		legend style={at={(0.03,0.97)}, anchor=north west},
    ]
    \addplot[color=red1,mark=diamond*] table[x=PIX] {Plots/single.gpu.data};%
    \addlegendentry{\protect\gls{CPU}}
   	\end{axis}
 	\end{tikzpicture}%
 	\caption{\protect\Gls{CPU} run time with increasing image size}%
	\label{fig:cpu_run_time}%
\end{figure}
Considering the numbers in the result one can see that the run times
grow linear to the image sizes. The mean shift algorithm has linear
complexity, hence its complexity can be written as $O(n)$. Doubling
the side length of the quadratic picture the run time increase by a
factor of 4. See e.g.  the run times for side length: (l=256, t=9) and
(l=512, t=36).
    
But this is obvious, as stated in \autoref{ch:mean_shift} for each
pixel of the image the mean shift vector has to be calculated, and if
one increases the number of pixels by a number $n$ we have to deal
with $n$ times longer run time. Such consideration have to be done in
the run-up to have a clue to which point the algorithm can be
accelerated. The problem has to be well understood.

The next important step is to find parts which can be offloaded to an
accelerator as the \gls{GPU}. A first good point for such parts is to
make a profiling of the application to find out the most
computationally intensive parts. It makes no sense to parallelise
functions which contribute only 1\% to the run time.
    
\section{Profiling the Original Code} % (fold)
\label{sec:run_time_analysis_of_the_original_code}
A good start point is to take a profiler and generate a profile of the
functions, recording their run time and call history. In this case a
statistical profiler is used which operates by sampling. The samples
are taken from the hardware performance counters which every modern
\gls{CPU} has built-in.
OProfile\footnote{http://oprofile.sourceforge.net/news/} a system wide
profiler for Linux was used to examine the run time of the
\gls{EDISON} program. The \autoref{tab:comped} shows the run time
analysis of \gls{EDISON}.

\begin{table}[ht]
  \centering
  \pgfkeys{/pgf/number format/.cd,fixed zerofill,precision=2}
  \pgfplotstabletypeset[
  % column type=r,
  % every row no 5/.style={
  % 		font=\textit,
  %   before row={\rowcolor{grey1}}
  % },
  columns/Self/.style={column type=r, column name={Self (\%)}},
  columns/Total/.style={column type=r, column name={Total (\%)}},
  columns/Symbol/.style={string type, column type=l, column name={Symbol}}
  ]{Tables/run.time.edison.data}
  \caption[\protect\Gls{EDISON} run time profile]{ \protect\Gls{EDISON} run time analysis}
  \label{tab:comped}
\end{table}

Before taking a closer look at the table, the columns have to be
explained. The column \emph{Total} shows how long the function plus
their child functions were executing. Whereas the column \emph{Self}
shows how long the function spent executing itself without the
execution time of their child functions. The column \emph{Symbol}
shows the function that was executed.

Now having a look at the table there is one function which is
executing 98.2\% of the run time, the function
\emph{uniformLSearch(double*, double*)}. In this function the
algorithm tries to find feature points that fall into the search
window with radius $h$ (see \autoref{sec:mean_shift}). This is the
starting point as it is the most computationally intensive and the
focus for parallelisation. There is a way to calculate to which extent
the particular function can be parallelised and which speedup one can
expect. Speedup is the ratio of run time when executing a program on a
single processor to the run time when using $n$ processors. Given $T_1$
the run time of a program on a single processor and $T_n$ the run
time of the same program on $n$ processors then
\begin{equation}\label{eq:speedup}
  S(n) = \frac{T_1}{T_n}
\end{equation}
is a measure for the speedup. One familiar law to calculate the how
much speedup can be obtained through parallelism is Amdahl's law.

\section{Amdahl's law}
\label{sec:amdahl_s_law}
Amdahl's law says supposing that 80\% of a computation can be
parallelised and 20\% can't, then even if the 80\% that is parallel
were run on an infinite number of processors the highest speedup that
one can achieve is 5. Generally speaking if a fraction $p$ of a
computation can be run in parallel and the rest must run serially,
Amdahl's law upper bounds the speedup by $1/(1-p)$.

The speedup of a application according to Amdahl is given by
\begin{equation}\label{eq:amdahl}
  S_{max}(n) = \frac{1}{(1-p) + \frac{p}{n}}
\end{equation} 
where $1$ is the execution unit time of the old computation, $(1-p)$
the inherently serial part and $p/n$ the parallel part divided by $n$
the number of processing units. When $n \rightarrow \infty$ then the
execution time approaches the time for executing the sequential
program fraction. So no matter how many processors one adds to the
system it will at least execute as long as the sequential program
fraction, which is an upper bound of speedup. A important addition is
that the sequential program fraction or serial processing percentage
is relative to the overall execution time using a single processor. It
is independent of the number $n$ of processors
\citeauthor{citeulike:3838998} \citep{citeulike:3838998}.

Assuming that the mean shift filtering can be parallelized with any number of
processing units one can calculate the maximum speedup achievable according to
Amdahl. Taking the parallel processing percentage from \autoref{tab:comp} for
the filtering step:
\begin{equation*}\label{eq:parallel}
  p = 99.3\% = 0.993 
  p\end{equation*}
we get

\begin{equation*}\label{eq:am0}
  S_{max}(n) = \frac{1}{(1-0.993)} = \frac{1}{0.007} = 142.857	
\end{equation*}

Applying the calculation with $n = 128$, the used \gls{GPU} has 128
processors one can achieve a speedup of:
\begin{equation*}\label{eq:g92sp}
  S_{max}(128) = \frac{1}{(1-0.993) + \frac{0.993}{128}} = \frac{1}{0.007 + 0.0077578125} = 67.76
\end{equation*}
One can see that even for very small serial processing percentage the speedup is
not very high. Therefore \citeauthor{citeulike:3732921} proposed a alternative
formulation where the fractions are now dependent on $n$. He assumed that for
larger problem, the fraction of a program to parallelisation increases. Thats
why Gustafson's law is often referred as a scaled speedup measure and Amdahl's
as a non scaled speedup measure. Given the serial $s'$ and parallel time $p'$ a
single processor would require $s' + p' \times n$ time to finish the execution.
The speedup is then given by:
\begin{equation}\label{eq:gus}
  S_{max}(n) = \frac{s' + p' \times n}{s' + p'} = n + ( 1 - n ) \times s'
\end{equation}
Applying the calculation with $n = 128$, the used \gls{GPU} has 128 processors
one can achieve a speedup according to Gustafson of:
\begin{equation*}\label{eq:g92sp}
  S_{max}(128) = 128 + (1 - 128) \times 0.007 = 127.11
\end{equation*}

It looks like that Amdahl's \& Gustafson's law are in contradiction but looking
more precise on both definitions one can see that both laws employ different
definitions for the fraction of the serial and parallel execution times. Amdahl
uses non scaled percentage and Gustafson scaled percentage. Mathematically they
are equal just two different formulations. The scaled percentage can be
transformed to a non scaled percentage where Gustafson's law gives the same
results as Amdahl's law \citep{citeulike:3838998}.

In summary one can expect more than a 100 fold speedup when parallelising the
filter step of the mean shift algorithm. The next steps will focus on analysing
how the mean shift filter can be decomposed to take advantage of multiple
processors.

\section{Data {\itshape{\&}} Task Parallelism} % (fold)
\label{sec:data_and_task_parallelism}
There are two ways to decompose an algorithm to take advantage of parallelism.
The first way is to decompose the algorithm into several tasks that can run
independently. Each task is performing independently different calculation on
the data. This characteristic is known as task parallelism. To know if two task
can run independent one can take \citeauthor{citeulike:6113408} conditions and
evaluate them.

Let $P_i$ and $P_j$ be two tasks of a program $P$. For $P_i$ let $I_i$ be the
input and $O_i$ the output data and for $P_j$ let $I_j$ be the input and $O_j$
be the output data, then $P_i$ and $P_j$ are independent if they satisfy the
following conditions
\citep{citeulike:3838998}:
\begin{subequations}\label{eq:bern_cond}
  \begin{align}
    I_j \cap O_i & = \emptyset \label{eq:bern0}\\
    I_i \cap O_j & = \emptyset \label{eq:bern1}\\
    O_i \cap O_j & = \emptyset \label{eq:bern2}
  \end{align}
\end{subequations}
The first two formulations \autoref{eq:bern0} and \autoref{eq:bern1} are stating
that the input of one task has to be independent from the other task. Otherwise
one task would have to wait for the output of the other task to continue.
Furthermore not only the inputs vs. the outputs have to be independent but also
the outputs of each task have to be independent. If the above conditions cannot
be applied to a program $P$ than it is likewise that race conditions and poor
performance can arise through e.g. excessive synchronisation overhead.


\subsection{Task Parallelism} % (fold)
\label{sub:task_parallelism}

Equipped with the knowledge how to identify independent parallel tasks, the mean
shift segmentation \autoref{alg:mss} can now be examined. Having a
look at the algorithm steps one can easily see that every task is violating
every condition stated above. The filtering step input data is dependent on the
\gls{RGB} to \gls{LUV} conversion output data. The segmentation step input data
is dependent on the filtering output data. So it does not matter how big an
image is or if it is a colour or grayscale image the mean shift segmentation
algorithm will always perform all from each other dependent calculations as
described in \autoref{alg:mss}. The longest path of dependent calculations is
the critical path. For the mean shift algorithm there exists no shorter path.

In summary one can discard the idea of task parallelism for the mean shift
segmentation algorithm. Thats why the focus of parallelisation is now on data
parallelism. 
% subsection task_parallelism (end)


\subsection{Data Parallelism} % (fold)
\label{sub:data_parallelism}
The second way of decomposing an algorithm is data decomposition. The data is
decomposed into chunks on which similar operations are being applied in such a
way that the different chunks can be operated on concurrently. The focus in data
parallelism is on the data structures which define the problem and not at the
tasks.

Focusing on data and having a look at the most computational intensive part (the
filtering step) one can see that each pixel of an image can be calculated
independently. The input and output pixels are independent and furthermore it
doesn't matter at which pixel one starts, the filtering step is deterministic,
starting from the same input will lead to the same output.

Another important aspect is how often the subtasks (where each subtask is
calculating one pixel) have to synchronise or communicate. An algorithm exhibits
fine-grained parallelism if the subtasks have to communicate many times,
coarse-grained parallelism if the subtasks do not communicate many times. In
this case the algorithm even exhibits embarrassingly parallel parallelism. The
subtasks never have to communicate or synchronise. Each subtask takes one or a
chunk of pixels applies the filtering steps and writes back the result. The
intermediate steps where the mean shift vector is moved over the spatial space
are also independent for each pixel.

The approach in parallelising the mean shift algorithm is to use a data
decomposition where each subtask is filtering a chunk or a pixel of the entire
image. Depending on the number of processing units one can choose an appropriate
chunk size to exploit every unit.

If the algorithm would only exhibit fine-grained parallelism the next steps
would be to identify groups to simplify the job of managing dependencies and
have a look at the ordering to satisfy constraints among tasks. Luckily here one
has only to deal with a embarrassingly parallel algorithm and can move to the
next step, identify how data is accessed and shared among subtasks.
% subsection data_parallelism (end)
% section data_and_task_parallelism (end)

\section{Data Flow} % (fold)
\label{sec:data_flow}

It is important to understand that inefficient data access can lead to very poor
performance on every processing unit especially on a \gls{GPU}. Inefficient data
access leads to an algorithm that is memory bound which means it doesn't matter
how much computational power the processing unit has it will be bound to the
memory performance (See \autoref{ch:literature_review} for \emph{arithmetic
intensity}). Therefore its crucial to understand the data and optimise it for
access. If done incorrectly tasks may get invalid data or it could lead to
excessive synchronisation overhead.

In \autoref{sub:task_parallelism} it was shown that the filtering step is
dependent on the colour conversion step where the output data of the colour
conversion step becomes the input data of the filtering step. Since this is done
sequentially there is no need to take care of some synchronisation. This scheme
is the same for the filtering and segmentation step. Since the filtering step is
done in parallel and the segmentation needs the output data form the filtering
step a barrier after the filtering step is needed.

Now to the interesting part the parallel filtering step. For the following
considerations the viewpoint will be a single processing unit. It is firstly
enough to consider only one unit and one pixel at a time since the \gls{GPU}
exhibits a \gls{SPMD} programming model and secondly the mean shift
algorithm is embarrassingly parallel. It is later easy to broaden the
examination to a cluster of processing units and chunks of pixels if its needed.

A processing unit firstly reads the corresponding pixel $x_n = (x_n^r, x_n^f)$
from the image. Then it moves the mean shift vector to the next pixel on the
path to the convergence point $z_n = (x_n^r, y_{conv}^f)$ by finding the feature
vector which falls into the search window. This new pixel is the basis for the
next iteration---calculation of the mean shift vector. On average of about
15--20 iterations the mean shift vector converges to the mean at the convergence
point \citep{DBLP:conf/eccv/ZhangKT06}. The whole path where the mean shift
vector is moving on is unpredictable. The vector can move over the whole image
or a few pixels, this is an important fact to consider when mapping the
algorithm to the \gls{GPU} because random access to the memory can decrease the
memory performance significantly. At last the convergence point is saved to the
$z_n = (z_n^r, z_n^f)$ data structure.

The segmentation step as stated above needs the data for the filtering step. The
only parallel portion of the algorithm is the filtering step so no further 
considerations will be made. 
% section data_flow (end)

\section{The next phase} % (fold)
\label{sec:analysis_summary}
So far the problem has been extensively analysed. The most compute intensive
parts have been identified and a preliminary speedup has been calculated
according to Gustafon's law. Furthermore the problem has been decomposed into
tasks that can execute concurrently using task respectively the data
decomposition. Additionally to the decomposition the dependencies between
processing units has been identified. 

The items  mentioned above will guide the design of the algorithm in the next
phase. Therefore it is important to have clearly understood the algorithm and
have done the decomposition in the right way. 

The decomposition has to be suitable for the target platform. Since the
\gls{GPU} is a \gls{SPMD} architecture, which means one program, the mean shift
algorithm is operating on many data the pixels. Above it was shown that the way
of data decomposition is preferable because (1) the pixels can be mapped 1:1 to
a processing unit on the \gls{GPU} and (2) there are enough pixel to keep the
\gls{GPU} busy. Furthermore the \gls{GPU} is a number cruncher and algorithms
doing heavy math or calculations can profit from the sheer computational power.

The decomposition has to be flexible, efficient and simple. A flexible setup
means that the amount of independent task can vary and the configuration how
they are arranged is as well selectable. This means it can be adapted to from
one to many processing units, the load can evenly be distributed among the
processing units which leads to good efficiency and scalability and the
decomposition can be parameterised in terms of units and problem size.
Additionally weak dependency in the decomposition makes load balancing easier.
Simple in this case means minimising the synchronisation and communication overhead
through the decomposition, which is in this case non-existent. 

The concurrency in the mean shift algorithm is now exposed and the algorithm can
now be taken into the next phase, designing the algorithm.
% section summary (end)

\section{The Algorithm Structure} % (fold)
\label{sec:introduction_design}

The next step involves the structuring of the algorithm to refine the design
going further into detail and move it closer to a program that can run on the
\gls{GPU}. The main concern here is how to map the concurrency identified in the
previous chapters to the processing units of a \gls{GPU}. Usually one wouldn't
go to much into hardware details in this phase because the design should be
flexible and portable, but without considering any hardware details of the
\gls{GPU} the algorithm is unlikely to run efficient. Therefore each design
decision made is based upon observance of hardware and programming model
(\gls{CUDA}) features. The strategy taken here is start from high level elements
of the programming model and go ever further into detail until the point is reached
where the algorithm can be implemented. 

\section{Programming Model} % (fold)
\label{sec:cuda_kernels}

In \autoref{sub:cuda_programming_model} it was explained that applications are
accelerated by executing data parallel portions of the algorithm on the
\gls{GPU} as \emph{kernels} which run in parallel on many threads exposing the
\gls{SPMD} nature of a \gls{GPU}. Additionally in \autoref{sub:data_parallelism}
the parallel portion of the algorithm was identified and hence the filtering
step will be implemented as a \gls{CUDA} kernel where it operates on different
data simultaneously. There is a strong dependency between the steps of the 
algorithm shown in \autoref{sec:data_and_task_parallelism} that have to be 
considered here as well. Before and after the filtering a colour conversion from
\gls{RGB} to \gls{LUV} and back is needed. These steps are not very compute 
intensive but why are they considered here?

An example is taken to have some numbers to juggle with. Supposing the algorithm
is split in 3 parts and the main part is executing 100 seconds and the two other
parts are executing 2 seconds. Additionally supposing that the main part can be
accelerated by a factor of 100 which means the main part takes now 1 second, the
most computational intensive part is not more the main part but rather the
remaining two parts. That means no matter how much faster the main part will be
the program will need always at least 4 seconds + accelerated main part to
execute. The solution to this problem would be to accelerate the two remaining
parts as well and thats what will be done for the mean shift algorithm. The two
colour space conversion steps will be as well offloaded to the \gls{GPU}. With
bigger and bigger images the \gls{CPU} will need more and more time to convert
from one colour space to another. 


All the other portions will be run on the \gls{CPU} because there are not(-yet) a
significant part of the run-time and are mostly post processing steps for the actual
algorithm. The \gls{CPU} will prepare, provide and gather the data and results
from the \gls{GPU}. 
% section cuda_kernels (end)

\section{Thread Batching} % (fold)
\label{sec:cuda_thread_hierarchy}

Fine grained, data parallel threads are the key concept of \gls{CUDA}. Several
of these threads are grouped together to a thread block. Thread blocks are again
grouped to so called grids and kernel is as a grid of thread blocks where data 
memory space is shared by all threads. The grid and the thread block can be 
organised either as a \gls{1D},\gls{2D} or as a \gls{3D} array, dependent on 
the problem structure adjusting to the state of the original data. This means 
that e.g. if the problem is \gls{3D} one could use the \gls{3D} configuration or
as in the mean shift algorithm case where the problem is a an \gls{2D} image than
obviously one should use the \gls{2D} array configuration.

Therefore the kernel used will be a \gls{2D} grid of \gls{2D} thread blocks. The
grid represents the whole image and the thread blocks are the image chunks that
are updated in parallel where a single thread is working on a single pixel. The 
exposed parallelism is directly mapped to the problem.  

For a efficient execution several $x$,$y$ combinations for the \gls{2D} array of the
thread blocks should be evaluated. The grid size in $x$ and in $y$ is fixed since
that are the dimensions of the analysed image. 

Since the algorithm is embarrassingly parallel there is no need to look for 
communication and synchronisation mechanisms in \gls{CUDA}.
% section cuda_thread_hierarchy (end)

\section{Device Memory space} % (fold) 
\label{sec:cuda_memory}

The memory space is a hierarchy of several memory types that can be accessed per
thread, block, grid and the host. The first most important memory is the global
memory that is the main data exchange place between the \gls{CPU} and \gls{GPU}. 
For the further considerations other memory types are not taken into account 
because (1) there are implementation specific and (2) using different memories
for the algorithm would not change the original nature of organisation and
decomposition. They can be later used to increase performance and data throughput. 

Coming back to global memory it is mainly used to read in the input data and
store the results which can be later either visualised using a graphics library
or transferred back to the \gls{CPU} where it can be further processed. 

Therefore the \gls{CPU} will run all preprocessing steps to the image and
transfer it to the global memory on the \gls{GPU}. After the filtering step
where the \gls{GPU} is reading continuously from the global memory for each
iteration of the filtering step the \gls{CPU} will fetch the data and
post process it. Further interaction between \gls{CPU} and \gls{GPU} are
non-existent.
% section cuda_memory (end)


\section{Warps} % (fold)
\label{sec:warps}

On the \gls{GPU} 32 threads build up a \gls{SIMD} unit in \slcsmallcaps{NVIDIA}
terms called warp, in that the same instruction is executed at a time but on
different data where on a \gls{CPU} the threads are scheduled independently. 

The consequence out of this is that if there is a branch the execution of the
two branch paths, when both are executed, are serialised. The only branch that
can happen in the mean shift algorithm is that several threads have found there
basin of attraction and other are still calculating. In this case there will no
serialisation happen rather there are threads that are idle.

This branching or dividing of a warp into idle and active threads is impossible
to circumvent. Because each image has different densities and the locations
of the densities vary there have to be threads that are idle, those e.g. that
are close to a density maximum and finish the calculation in few iterations,
and threads that are active, those that were further apart of the density maximum
and need a higher number of iterations. This means threads in a warp that
are gone to idle state will stay idle and active threads will do their 
calculations no rearranging of threads or reassignment of data will be done. 

In summary it can be said that each thread is executing the same program. The 
execution time of each thread highly depends on the location of the thread in
the grid and the location of the density maximums. The different execution times
are visible directly in the final iteration count. 
% section warps (end)

\section{Program Flow} % (fold)
\label{sec:program_flow}

The point of view will be single thread since all threads are doing the same. 
After the \gls{CPU} has loaded the picture it preprocesses it by converting
the \gls{RGB} pixel values to \gls{LUV}. After the conversion the \gls{CPU}
transfers via \gls{DMA} the image to the \gls{GPU}.

A thread in the grid having a unique id can now fetch the corresponding pixel
according to the id. Equipped with the colour data the thread can now calculate
the mean shift vector until convergence. While calculating, the thread will 
access various pixel in the image that are read-only in the global memory. The 
found convergence point is saved to a new buffer in the global memory.

Finally the \gls{CPU} fetches the convergence points, converges the image from 
the \gls{LUV} colour space to the \gls{RGB} space. The post processing step
involves the segmentation and pruning of regions smaller than a specified 
variable. The result is a segmented image. 
% section program_flow (end)


\section{Summary} % (fold)
\label{sec:design_summary}
After an extensive analysis it was no problem to put the single parts together
to form a program. The design phase usually does not consider any hardware but
in this case where an efficient algorithm should be mapped to the \gls{GPU} it
is crucial to know the architecture and the features the hardware offers. 
Furthermore it is important to have a look at the data. A good design follows
from good data design. In this case a data decomposition was chosen and hence the 
program had to accommodate to this circumstance. The data decomposition was not 
chosen arbitrarily it was chosen with the ulterior motive that the data fits
$1:1$ to the hardware features. 

In summary, data is more important than code, because crappy data design leads
to low bandwidth usage and the computational units starve because the data
is not in place (memory bound). Writing a good algorithm means to understand the 
data. 
% section summary (end)




