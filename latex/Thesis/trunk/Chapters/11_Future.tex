%!TEX TS-program = pdflatex
%!TEX encoding = UTF-8 Unicode
%!TEX TS-options = -halt-on-error
\chapter{Conclusions \textit{\&} Further Work} % (fold)
\label{ch:further_work}

% conclusions: conclusions should be basde on an in depth critical analysis of
% 			 			 the information presented in the dissertation and should be 
%							 related to the objectives stated in the introduction


% further work: identify specific additional investigation that is required to 
%							  be carried out. 


% do not simply summarise the dissertation
% do not recapitulate the analysis or discussion
% do not introduce new ideas

% identify specific points that have been clarified or discovered, and specific
% actions to be taken

% identify specific additional investigation that is required (and why)
The field of \gls{GPU} computing is nowadays very broad. Starting from dense
linear algebra, n-body, finite difference over graphics, image processing to
computational modeling in science, engineering and finance the \gls{GPU} is
existent in all this fields. A lot of algorithms profited from using the
\gls{GPU} as an accelerator and many will profit too. There is a high demand on
performance and computational power for scientific algorithms. Developers search
for new architectures to solve problems in shorter time and the \gls{GPU} is one
promising candidate because of its incredible raw power. 

New architectures meant new programming models and in the case of the \gls{GPU}
the first programming model was not build for general purpose computing and
developers were accidently forced to use \gls{3D} \glspl{API}
(OpenGL\footnote{http://www.opengl.org},
DirectX\footnote{http://www.microsoft.com}) to accomplish task like matrix
multiplication or fast fourier transformation. These \glspl{API} implied some
limitations and raised some myth for the new era of \gls{GPGPU} when the switch
from fixed pipelines to fully programmable pipelines happen. 

Developers were concerned about the programming models will still have the
limitations as the former had. The feasibility study showed that all the odds
and ends are not more valid for the new architectures and programming models.
This new sophisticated programming environments, like \gls{CUDA} for the
{\slcsmallcaps{NVIDIA}} \glspl{GPU}, offer a complete, easy and new interface to
the hardware to exploit parallelism as much as possible that make \gls{GPGPU}
more feasible.

With the advent of these new programming environments more and more developers
are using \glspl{GPU} for general purpose computing. Many of them come from a
world where everything is sequentiel and struggle with thinking in parallel.
Developers often only needed to compile their code with another compiler to
port the code to another architecture. For example porting code from x86
to PowerPC or x86 to \gls{SPARC}  was easy enough if it was standard C or C++
without any special operating system calls. But the situation for parallel
processors like the Cell, Niagara and here the \gls{GPU} is completely
different. One has to identify the critical parts and explicitly offload tasks
to the \gls{GPU}. It is obviuos that this is not the only step to do.
Furthermore there are other facts which have to be considered, like
load-balancing, deadlocks, concurrency, cache coherency and other. A workflow
was presented throughout this thesis by the means of an algorithm how all this
neccessary steps were untertaken.

The porting step is not the last when considering high performance code on a
\gls{GPU}. The next big thing is to optimize the code to some extent to run
efficiently on the hardware. The main cause of unefficient code is bad data
design. It is the biggest problem of an alogirthm when trying to attain maximum
performance. If the algorithm is memory bound and not computational bound the
limiting factor or bottleneck is the bandwidth to the memory. Good data design
is essential for good, performant code. The data and how it is used must be
known in order to effectively design and optimize a application for a \gls{GPU}
or other highly parallel systems like the Cell processor
\citep{citeulike:80546}. A simple computationally bound algorithm can be easily
designed in a way that it becomes memory bound if data arrangement and access is
neglected by the developer. A good data design is essential for any massively
parallel system.


\section{Further Work} % (fold)
\label{sec:further_work}
For the mean shift algorithm there exist several acceleration teqhniques. These
techniques could be applied to the algorithm implemented in this thesis to
accelerate the algoirthm to some extent. There were several papers that describe
the techniques in depth. All of them have on thing in common, they degrade the
quality of the segmented image whereas the algorithm used throughout this thesis
is the most accurate but also the slowest. But not all techniques imply a very high
quality loss, often not seen by the human eye and can be used for tracking or
cutout filters for image processing. 

Further work could also extend the existing algorithm to segment not only pictures 
but also videos where tracking is a big field of interest. For image processing
software different filters could be implemented like ``paint by numbers'' or 
``comic effect'' and many more. Photoshop \gls{CS4} from Adobe e.g, is making heavy
use of the \gls{GPU} already and has a plugin architecture for developing custom
filters.  
 
\subsubsection{Fermi} % (fold)
\label{ssub:fermi}
Fermi is a announced but yet not released new groundbreaking architecture from
{\slcsmallcaps{NVIDIA}}. Many of the limitations with the current hardware like 
non \gls{ECC} memory, single kernel, no C++ support, small main memory, register
pressure and so on are being improved or completely eliminated. It features more
shared memory, register, streaming multiprocessors and many more especially
designe to meet the requirements of the \gls{HPC} and general purpose computing
communities. It would be worthwhile to run the algorithm on this new architecture
to see how the promised scaling without recompiling is really doing. But for more
performance it should be recompiled and adapted to all the new features available.

The problem with new hardware architectures is they have to be accepted by the
developers to succeed. One way to achieve this is to have an easy programming
model which adapts easily to common apis and existing systems. There are rarely
branches of industry which want to, or are able to rewrite their code for
bleeding edge architectures like the cell or \glspl{GPU} if the outcome is only
2x faster code. In this case many just extend their cluster with another bunch
of cheap systems. A factor of 10 is an undocumented limit when to invest in
other, faster hardware.

OpenCL, \gls{CUDA} and other programming model make a promising start in the field
of massively parallel computers. More and more developers will or have to adapt to
parallel architectures and parallel programming. Such programming models make 
the life easier and can help to understand parallelism.
% subsubsection fermi (end)

% section further_work (end)



% chapter future_work (end)

