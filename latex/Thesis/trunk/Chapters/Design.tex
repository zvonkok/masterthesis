%!TEX TS-program = pdflatex
%!TEX encoding = UTF-8 Unicode

\chapter{Mean Shift Algorithm Design} % (fold)
\label{cha:algorithm_design}
The previous chapter explained and showed what is gonna be done to the algorihtm
to have it running on a \gls{GPU}. In this chapter it will be explained how all
the proposals are realized and how everything is put together. The result is an
scalable and efficient realization of the mean shift algorithm on the \gls{GPU}.

The next step involves the structuring of the algorithm to refine the design
going further into detail and move it closer to a program that can run on the
\gls{GPU}. The main concern here is how to map the concurrency identified in the
previous chapters to the processing units of a \gls{GPU}. Usually one wouldn't
go to much into hardware details in this phase because the design should be
flexible und portable, but without considering any hardware details of the
\gls{GPU} the algorithm is unlikely to run efficient. Therefore each design
decision made is based upon observance of hardware and programming model
(\gls{CUDA}) features. The strategy taken here is start from high level elements
of the programming model and go ever further into detail until the point is reached
where the algorithm can be implemented. 

\section{Programming Model} % (fold)
\label{sec:cuda_kernels}

In \autoref{sub:cuda_programming_model} it was explained that applications are
accelerated by executing data parallel portions of the algorithm on the
\gls{GPU} as \emph{kernels} which run in parallel on many threads exposing the
\gls{SPMD} nature of a \gls{GPU}. Additionally in \autoref{sub:data_parallelism}
the parallel portion of the algorithm was identified and hence the filtering
step will be implemented as a \gls{CUDA} kernel where it operates on different
data simultaneously. Since the most compute intensive parts are in the filtering
step no further kernels are required. 

All the other portions will be run on the \gls{CPU} because there are not a
significant part of the run-time and are mostly preparing steps for the actual
algorithm. The \gls{CPU} will prepare, provide and gather the data and result 
from the \gls{GPU}. 
% section cuda_kernels (end)

\section{Thread Batching} % (fold)
\label{sec:cuda_thread_hierarchy}

Fine grained, data parallel threads are the key concept of \gls{CUDA}. Several
of these threads are grouped together to a thread block. Thread blocks are again
grouped to so called grids and kernel is as a grid of thread blocks where data 
memory space is shared by all threads. The grid and the thread block can be 
organized either as a \gls{1D},\gls{2D} or as a \gls{3D} array, dependent on 
the problem structure adjusting to the state of the original data. This means 
that e.g. if the problem is \gls{3D} one could use the \gls{3D} configuration or
as in the mean shift algorithm case where the problem is a an \gls{2D} image than
obviuosly one should use the \gls{2D} array configuration.

Therefore the kernel used will be a \gls{2D} grid of \gls{2D} thread blocks. The
grid represents the whole image and the thread blocks are the image chunks that
are updated in parallel where a single thread is working on a single pixel. The 
exposed parallelity is directly mapped to the problem.  

For a efficient execution several $x$,$y$ combinations for the \gls{2D} array of the
thread blocks should be evaluated. The grid size in $x$ and in $y$ is fixed since
that are the dimensions of the analyzed image. 

Since the algorithm is embarassingly parallel there is no need to look for 
communication and sychronization mechanisms in \gls{CUDA}.
% section cuda_thread_hierarchy (end)

\section{Device Memory space} % (fold) 
\label{sec:cuda_memory}

The memory space is a hierarchy of several memory types that can be accessed per
thread, block, grid and the host. The first most important memory is the global
memory that is the main data exchange place between the \gls{CPU} and \gls{GPU}. 
For the further considerations other memory types are not taken into account 
because (1) there are implementation specific and (2) using different memories
for the algorithm would not change the original nature of organization and
decomposition. They can be later used to increase performance and data throughput. 

Coming back to global memory it is mainly used to read in the input data and
store the results which can be later either visualized using a graphics library
or transferred back to the \gls{CPU} where it can be further processed. 

Therefore the \gls{CPU} will run all preprocessing steps to the image and
transfer it to the global memory on the \gls{GPU}. After the filtering step
where the \gls{GPU} is reading continuosly from the global memory for each
iteration of the filtering step the \gls{CPU} will fetch the data and
postprocess it. Further interaction between \gls{CPU} and \gls{GPU} are
non-existent.
% section cuda_memory (end)


\section{Warps} % (fold)
\label{sec:warps}

On the \gls{GPU} 32 threads build up a \gls{SIMD} unit in \slcsmallcaps{NVIDIA}
terms called warp, in that the same instruction is executed at a time but on
different data where on a \gls{CPU} the threads are scheduled independently. 

The consequence out of this is that if there is a branch the execution of the
two branch paths, when both are executed, are serialized. The only branch that
can happen in the mean shift algorithm is that several threads have found there
basin of attraction and other are still calculating. In this case there will no
serialization happen rather there are threads that are idle.

This branching or dividing of a warp into idle and active threads is impossible
to circumvent. Because each image has different densities and the locations
of the densities vary there have to be threads that are idle, those e.g. that
are close to a density maximum and finish the calculation in few iterations,
and threads that are active, those that were further apart of the density maximum
and need a higher number of iterations. This means threads in a warp that
are gone to idle state will stay idle and active threads will do their 
calculations no rearranging of threads or reassignment of data will be done. 

In summary it can be said that each thread is executing the same program. The 
execution time of each thread highly depends on the location of the thread in
the grid and the location of the density maximums. The different execution times
are visible directly in the final iteration count. 
% section warps (end)

\section{Program Flow} % (fold)
\label{sec:program_flow}

The point of view will be single thread since all threads are doing the same. 
After the \gls{CPU} has loaded the picture it preprocesses it by converting
the \gls{RGB} pixel values to \gls{LUV}. After the conversion the \gls{CPU}
transfers via \gls{DMA} the image to the \gls{GPU}.

A thread in the grid having a unique id can now fetch the corresponding pixel
according to the id. Equipped with the color data the thread can now calculate
the mean shift vector until convergence. While calculating the thread will 
access various pixel in the image that are read-only in the global memory. The 
found convergence point is saved to a new buffer in the global memory.

Finally the \gls{CPU} fetches the convergence points, converges the image from 
the \gls{LUV} color space to the \gls{RGB} space. The postprocessing step
involves the segmentation and pruning of regions smaller than a specified 
variable. The result is a segmented image. 
% section program_flow (end)


\section{Summary} % (fold)
\label{sec:design_summary}
After a extensive analysis it was no problem to put the single parts together
to form a program. The design phase usually does not consider any hardware but
in this case where an efficient algorithm should be mapped to the \gls{GPU} it
is crucial to know the architecture and the features the hardware offers. 
Furthermore it is important to have a look at the data. A good design follows
from good data design. In this case a data decomposition was choosen and hence the 
program had to accomodate to this circumstance. The data decomposition was not 
choosen arbitrarily it was choosen with the ulterior motive that the data fits
$1:1$ to the hardware features. 

In summary, data is more important than code, because crappy data design leads
to low bandwidth usage and the computational units starve because the data
is not in place (memory bound). Writing a good algorithm means to understand the 
data. 
% section summary (end)








% chapter algorithm_design (end)
