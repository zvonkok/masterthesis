\chapter{Performance {\itshape{\&}} Scalability} % (fold)
\label{cha:performance_and_scalability_}


To evaluate the performance and the scalability of the implemented mean shift
algorithm several benchmarks will be made. Firstly the algorithm is tested 
against different image sizes. After that a multiple \gls{GPU} version is 
evaluated and tested with the same image sizes as in the previous test. Lastly
the hardware parameters of the \gls{GPU} like the core clock and memory clock
are manipulated to get a clue if the algorithm is memory or computational bound. 
In the case that the algorithm is memory bound one should try to decrease the
communication and if the algorithm is computational bound a restructuring of 
the algorithm could help here. All in all its interesting to see how an algorithm
behaves at different circumstances. In general all runs were performed 20 times
in a row and the mean was taken as the final result.

\section{Varying the Image Size} % (fold)
\label{sec:varying_the_image_size}
The first benchmark varies the image size from 128 $\times$ 128 pixels to 2688
$\times$ 2688 pixels. The image side length is incremented by 128 pixels. The
\autoref{fig:gpu_speedup} shows the \gls{GPU} runtime and the speedup compared
to the \gls{CPU} run time. The \gls{CPU} run time was not included in
\autoref{fig:gpu_speedup} because the range of \gls{CPU} values is 100$\times$ 
larger and the \gls{GPU} values would not be visible. 
\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
	
		\pgfplotsset{
			every axis legend/.append style={at={(0.02,0.98)}, anchor=north west}
		}
		\pgfplotstableread{Plots/single.gpu.data}\tableA
		
    \begin{axis}[
			% colormap/violet,
			% legend columns=2,
      % smooth,
      % stack plots=y,
      area style,
      %ybar, 
			%bar width=4pt,
      width=0.88\textwidth,
      height=5cm,
      xtick={128,384,...,2688},
%			xticklabel={$2^14$, $2^15$},
      axis x line=bottom,
      axis y line=left,
      % xmin=128, xmax=2688, 
      %ymin=0, 
			ymax=7000,
      xlabel=Image size, ylabel={Milliseconds [ms]}, 
			% enlargelimits=0.03,
      ymajorgrids ]
  %    \addplot%[color=plotcolor0!50!black,fill=plotcolor0]
  %    table[x=PIX,y={CPU [ms]}] {Plots/cpu_gpu_runtime.data};%
	%		\addlegendentry{CPU}
  %    \closedcycle;

      \addplot[color=pl1,fill=pf1,mark=none] table[x=PIX,y={GPU}] from \tableA;

			\addlegendentry{\slcsmallcaps{GPU}}

    \end{axis}

		\begin{axis}[
     	width=0.88\textwidth,
      height=5cm,
      xtick={128,384,...,2688},
      axis x line=none,
      axis y line=right,
      xmin=128, xmax=2688, 
      ymin=100,
 			ymax=190,
  %    xlabel=Image size, 
			ylabel={Speedup}, 
	%		enlargelimits=0.03,
      ymajorgrids ]
      \addplot[color=plot0,mark=none] table[x=PIX,y={Speedup}] from \tableA;
%			\addlegendentry{CPU}
    \end{axis}

  \end{tikzpicture}%
  \caption{GPU runtime and speedup dependent on the image size}%
	\label{fig:gpu_speedup}%
\end{figure}
Its interesting to see that the speedup is increasing faster at the beginning
and than to level off at about 160. This is no surprise because every software
respectively hardware has a ramp-up phase. This ramp-up phase, where no
computation is performed, often involves allocation and initialization of data
and hardware and additionally on this platform the movement of data from the
host to the \gls{GPU} buffers. With smaller image sizes the time for the ramp-up
phase is a significant amount of the complete run-time. With bigger and bigger
image sizes the amount of the ramp-up phase to the complete run-time becomes 
smaller and hence the speedup increases. 

\subsection{Linearity} % (fold)
\label{ssub:linearity}
Another fact to consider is how the algorithm is scaling with increasing image
sizes. In \autoref{ch:algorithm_analysis} it was shown that the algorithm
exposes linear complexity $O(n)$ and it is interesting to see how the
implemented algorithm behaves in terms of linearity. An algorithm is linear when
the two metrics for measuring the performance here the image size and the
run-time are proportional. The image size is proportional to the run-time if 
they have a constant ratio. If one variable increases by an factor $x$ than the
proportional variable increases by that factor too.
\begin{figure}[ht]
  \centering
	
%	\pgfplotsset{every axis x label/.style={at={(1,0)}, above}}
%  \pgfplotsset{every axis y label/.style={at={(0,1)}, left}}

	\pgfplotstableread{Plots/single.gpu.data}\tableA

  \begin{tikzpicture}
		\begin{axis}[
     	width=0.88\textwidth,
      height=5cm,
			ytick={0,0.5,1,1.5},
      xtick={128,384,...,2688},
      axis x line=bottom,
      axis y line=left,
%      xmin=0, xmax=2796, 
      ymin=0, ymax=2,
      xlabel=Image Size,
      ylabel=Normalized Size/Time Ratio,
			enlargelimits=0.01,
    %  ymajorgrids 
			]
     
 			\addplot table[x=PIX,y=GPU_Norm_Ratio] from \tableA;
 			\addplot table[x=PIX,y=CPU_Norm_Ratio] from \tableA;

			\addlegendentry{\slcsmallcaps{GPU}}
			\addlegendentry{\slcsmallcaps{CPU}}

    \end{axis}

  \end{tikzpicture}%
	\caption{Normalized size/time ratios}%
	\label{fig:linearity}
 \end{figure}

The \autoref{fig:linearity} shows the normalized image size per time ratios in
the interval $[0,1]$. As it can be seen the ratio for the \gls{CPU} and the
\gls{GPU} are increasing similar to the speedup because of the before mentioned
ramp-up phase. The ratio increases until it reaches the right endpoint of the
interval. If the image size is doubled the run-time is doubled (image size
$\propto$ run-time) which means the algorithm scales linearly.
% subsubsection linearity (end)
% section varying_the_image_size (end)


\section{Multiple gpus} % (fold)
\label{sec:multiple_gpus}
The previous section analyzed how the implemented algorithm scales over the
image size. Another important attribute is how the algorithm scales if another
\gls{GPU} is added for computation. A second identical \gls{GPU} was used to
generate the results shown in \autoref{fig:multi_gpu_speedup}.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
	
		\pgfplotsset{
			every axis legend/.append style={at={(0.02,0.98)}, anchor=north west}
		}
		\pgfplotstableread{Plots/multi.gpu.data}\tableA
		
    \begin{axis}[
			% colormap/violet,
			% legend columns=2,
      % smooth,
      % stack plots=y,
      % area style,
      ybar, 
			bar width=4pt,
      width=0.88\textwidth,
      height=5cm,
      xtick={128,384,...,2688},
      axis x line=bottom,
      axis y line=left,
      xmin=0, xmax=2796, 
      %ymin=0, ymax=1100,
      %xlabel=Image size, 
			%ylabel={Milliseconds [ms]}, 
			enlargelimits=0.03,
      ymajorgrids 
			]

      \addplot table[x=PIX,y=1GPU] from \tableA;
			\addplot table[x=PIX,y=2GPU] from \tableA;
			
			\addlegendentry{\slcsmallcaps{1 GPU}}
			\addlegendentry{\slcsmallcaps{2 GPUs}}

    \end{axis}

		\begin{axis}[
     	width=0.88\textwidth,
      height=5cm,
      xtick={128,384,...,2688},
      axis x line=none,
      axis y line=right,
      xmin=0, xmax=2796, 
      ymin=0, ymax=4,
  %    xlabel=Image size, 
			ylabel={Speedup}, 
			enlargelimits=0.03,
      ymajorgrids ]
      \addplot%[color=plotcolor0!50!black,fill=plotcolor0]
      table[x=PIX,y={GPU_Speedup}] from \tableA;
%			\addlegendentry{CPU}
%      \closedcycle;
    \end{axis}

  \end{tikzpicture}%
  \caption{Multiple GPU(s) runtime and speedup dependent on the image size}%
	\label{fig:multi_gpu_speedup}%
\end{figure}

As stated in the previous sections the ramp-up phase is responsible that the
algorithm is not scaling with small image sizes. Its again clearly identifiable
in the \autoref{fig:multi_gpu_speedup}. It's safe to assume that the multiple
\gls{GPU} run-times are proportional to the image size as in
\autoref{fig:linearity} seen for one \gls{GPU} since the run-times are 2$\times$
faster but the ratio will nevertheless still be a constant. For the precise
numbers see \autoref{tab:multi_gpu_run}. This section showed that the algorithm
scales perfectly with multiple devices and it would be no surprise to see a
speedup of 3$\times$ if a third \gls{GPU} were added to the system. Additionally
the \autoref{fig:multi_gpu_cpu_speedup} shows the speedups of 1 \gls{GPU} and 2
\glspl{GPU} compared to the \gls{CPU} run-times.

\begin{figure}[ht]
  \centering
	
	\pgfplotsset{every axis x label/.style={at={(1,0)}, above}}
 	\pgfplotsset{every axis y label/.style={at={(0,1)}, left}}

	\pgfplotstableread{Plots/single.gpu.data}\tableA
	\pgfplotstableread{Plots/multi.gpu.data}\tableB	

  \begin{tikzpicture}
		\begin{axis}[
			area style,
     	width=\textwidth,
      height=5cm,
%			ytick={0,0.5,1,1.5},
      xtick={128,384,...,2688},
      axis x line=bottom,
      axis y line=left,
      xmin=128, xmax=2816, 
      ymin=0, ymax=350,
      xlabel=$x$,
      ylabel=$y$,
		%	enlargelimits=0.01,
    %  ymajorgrids 
			]
     
 			\addplot[color=plot1!80!black,fill=plot1!40!white] table[x=PIX,y=CPU_Speedup] from \tableB;
 			\addplot[color=plot0!80!black,fill=plot0!40!white] table[x=PIX,y=Speedup] from \tableA;

			\addlegendentry{\slcsmallcaps{1 GPU}}
			\addlegendentry{\slcsmallcaps{2 GPUS}}

    \end{axis}

  \end{tikzpicture}%
	\caption{{\ssmallcaps{Gpu}} speedups compared to the {\slcsmallcaps{CPU}} 
					 run-times over image size}%
	\label{fig:multi_gpu_cpu_speedup}
\end{figure}

% section multiple_gpus (end)

\section{Overclocking the gpu} % (fold)
\label{sec:overclocking_the_gpu}
The \gls{GPU} has three \glspl{clock} that can be over-clocked to achieve higher
performance. The first is the core \glspl{clock}, the second \gls{clock} is
the memory clock and the third clock is the \gls{shader} \gls{clock} where the
shader clock typically moves synchronically with the core as they work on a set
ratio. For example the used \emph{GeForce 8800 GTS 512} features a 650 \gls{MHz}
core, a 1620 \gls{MHz} shader and a 970 \gls{MHz} memory clock. This means this
particular graphics card uses a core to shader \gls{clock} multiplier of
2.5$\times$. Increasing the core \gls{clock} means increasing the shader clock
at the same time. Therefore only the core and memory clock will be considered
for the experiments.

The reason why someone would over-clock a \gls{GPU} is at first hand to increase
the performance and on the second hand to find out if an algorithm is
computational or memory bound by experiment.

Therefore several experiments will be undertaken. One experiment will involve
the change of the core \gls{clock}, second the change of the memory \gls{clock}
and lastly the combination of both \glspl{clock}. All experiments were done with
a free tool, \emph{NVClock}
\footnote{http://www.linuxhardware.org/nvclock/}. NVClock is a small utility
that allows to over-clock {\slcsmallcaps{NVIDIA}} based \glspl{GPU} and adjust the fan speed. This
is important since the \gls{GPU} will run out of specification and could overheat.
The experiments will begin with changing the memory clock and evaluation of the
results. 


\subsection{Increasing the Memory Clock} % (fold)
\label{sub:increasing_the_memory_clcok}
For the first experiment the memory clock will be changed by steps of 100 \gls{MHz}.
As a starting point the default 972 \gls{MHz} of the memory clock will be used 
and the core clock will be fixed at the default of 648 \gls{MHz}. Experiments 
have shown that the maximal achievable memory clock is 1101.600 \gls{MHz}. 
All clocks were read out from hardware with \emph{NVClock}. Additionally it must
be noted that all frequency settings were not accepted one-on-one which means an 
increase per software of 100 \gls{MHz} doesn't mean an increase of 100 \gls{MHz}
in hardware due to hardware restrictions. The \autoref{tab:mem_sw_hw} shows the 
frequency set in software and the resulting real frequency in hardware. 

The test was performed with three different image sizes: 256$\times$256 pixel, 
1024$\times$1024 pixel and 2048$\times$2048 pixel. The \autoref{fig:mem_clk_256}
shows the run-time dependent on the memory clock.

\begin{figure}[ht]
  \centering

  \pgfplotsset{every axis x label/.style={at={(1,0)}, above}}
  \pgfplotsset{every axis y label/.style={at={(0,1)}, left}}

  \pgfplotstableread{Plots/mem.single.gpu.data}\tableA

  \begin{tikzpicture}
    \begin{axis}[width=\textwidth, height=5cm,
      axis x line=bottom,
      axis y line=left,
      xmin=972, xmax=1110, 
    	ymin=70,
			ymax=73.5,
      xlabel=$x$,
      ylabel=$y$]
      \addplot table[x=MEMCLK,y=256] from \tableA;
   \end{axis}
 \end{tikzpicture}%
 \label{fig:mem_clk_256}%
 \caption{Image 256x256 increasing the memory clock}
\end{figure}

A straight line indicates that the algorithm is not memory bound. It does not
make any difference how fast the memory one make the run-time will not decrease. 
Additionally it can be concluded that the algorithm is computational bound. This
will be proven in the next section when the core clock is manipulated. The image
sizes 1024$\times$1024 and 2048$\times$2048 pixel show the same behavior. For 
the complete list of numbers see \autoref{tab:mem_clk_gpu}.

% subsection increasing_the_memory_clcok (end)

\subsection{Increasing the Core Clock} % (fold)
\label{sub:increasing_the_core_clock}
For the second experiment the core clock will be changed by steps of 100
\gls{MHz} and as a dependency the shader clock as well. As a starting point the
default 648 \gls{MHz} of the core clock will be used and the memory clock will
be fixed at the default of 972 \gls{MHz} because the previous section has shown
that the algorithm is not memory bound and the increase of the memory clock has
no effect on the run-time. Experiments have shown that the maximal achievable
core clock is 783 \gls{MHz}. All clocks were read out from hardware with
\emph{NVClock}. The \autoref{tab:core_sw_hw} shows the frequency set in software
and the resulting real frequency in hardware.

The test was performed with three different image sizes: 256$\times$256 pixel, 
1024$\times$1024 pixel and 2048$\times$2048 pixel. The \autoref{fig:core_clk_256}
shows the run-time dependent on the core clock. Additionally the normalized
ratio {\color{red}{clock/time}} is depicted as well. 

\begin{figure}[ht]
  \centering

  \pgfplotsset{every axis x label/.style={at={(0.96,0)}, above}}
  \pgfplotsset{every axis y label/.style={at={(0,1)}, left}}


  \pgfplotstableread{Plots/clock.single.gpu.256.data}\tableA

  \begin{tikzpicture}
		\begin{axis}[
			cycle list name=exotic,
      % smooth, 
      % stack plots=y,
     % area style,
    %  ybar, bar width=10pt,
      % line width=0.7pt, %0.5pt default
      width=\textwidth,
      height=5cm,
      % xtick={128,384,...,2688},
      ytick={50,60,70,80},
      axis x line=bottom,
      axis y line=left,
      xmin=648, xmax=783, 
      ymin=50,
			ymax=85,
      xlabel=$x$,
      ylabel=$y$,
 %     enlarge x limits=0.1,
      % ymajorgrids
      ]
      \addplot table[x=CLK,y=972.000] from \tableA;
   \end{axis}	
	\begin{axis}[
	      % smooth, 
	      % stack plots=y,
	     % area style,
	    %  ybar, bar width=10pt,
	      % line width=0.7pt, %0.5pt default
	      width=\textwidth,
	      height=5cm,
	      % xtick={128,384,...,2688},
%	      ytick={40,50,60,70,80},
	      axis x line=none,
	      axis y line=right,
	      xmin=648, xmax=783, 
				ymin=0,
				ymax=2,
%	      xlabel=$x$,
%	      ylabel=$y$,
%	      enlarge x limits=0.1,
	      % ymajorgrids
	      ]
	      \addplot table[x=CLK,y=CT_NRatio] from \tableA;
				\addlegendentry{cl}
	   \end{axis}



 \end{tikzpicture}%
 \label{fig:core_clk_256}%
 \caption{Image 256x256 varying the core clock with fixed mem clock}
\end{figure}

It's visible that the run-time is decreasing proportionally with the increase of
the core-clock. This means that the algorithm is heavily computational bound.
The run-time is determined in this implementation solely by the speed of the
\gls{GPU}. One way to get over this barrier is to (1) minimize the instructions
executed by the \gls{GPU} by restructuring the code or (2) adopting a another
algorithm for finding feature point that fall into the search window, see
\autoref{ch:future_work} for enhancements that could be made to accelerate the
search. Additionally, the image sizes 1024$\times$1024 and 2048$\times$2048
pixel show the same behavior.
For completeness the next section shows the case when two \glspl{GPU} are 
overclocked.

\subsubsection{Multiple gpus overclocked} % (fold)
\label{ssub:multiple_gpus_overclocked}
It's no surprise to see almost a identical diagram compared to the previous
section. As the algorithm scales well with multiple \gls{GPU} (see
\autoref{sec:multiple_gpus}) and with the frequency of the core clock one can 
assume to see the same sequence of run-times just cut by half. The
\autoref{fig:clock_multi_256} shows that, additionally attention should be paid
to the shape of the curve as its the same as in \autoref{fig:core_clk_256}. 

\begin{figure}[ht]
  \centering

  \pgfplotsset{every axis x label/.style={at={(0.96,0)}, above}}
  \pgfplotsset{every axis y label/.style={at={(0,1)}, left}}


  \pgfplotstableread{Plots/clock.multi.gpu.256.data}\tableA

  \begin{tikzpicture}
    \begin{axis}[
			cycle list name=exotic,
      % smooth, 
      % stack plots=y,
     % area style,
    %  ybar, bar width=10pt,
      % line width=0.7pt, %0.5pt default
      width=\textwidth,
      height=5cm,
      % xtick={128,384,...,2688},
     	ytick={25,30,35,40},
      axis x line=bottom,
      axis y line=left,
      xmin=648, xmax=783, 
      ymin=25,
			ymax=45,
      xlabel=$x$,
      ylabel=$y$,
 %     enlarge x limits=0.1,
      % ymajorgrids
      ]
      \addplot table[x=CLK,y=972.000] from \tableA;
   \end{axis}	
	\begin{axis}[
	      % smooth, 
	      % stack plots=y,
	     % area style,
	    %  ybar, bar width=10pt,
	      % line width=0.7pt, %0.5pt default
	      width=\textwidth,
	      height=5cm,
	      % xtick={128,384,...,2688},
%	      ytick={40,50,60,70,80},
	      axis x line=none,
	      axis y line=right,
	      xmin=648, xmax=783, 
	      ymin=0,
				ymax=2,
%	      xlabel=$x$,
%	      ylabel=$y$,
%	      enlarge x limits=0.1,
	      % ymajorgrids
	      ]
	      \addplot table[x=CLK,y=CT_NRatio] from \tableA;
				\addlegendentry{cl}
	   \end{axis}



 \end{tikzpicture}%
 \label{fig:clock_multi_256}%
 \caption{Image 256x256 varying the core on two GPUS}
\end{figure}

% subsubsection multiple_gpus_overclocked (end)
% subsection increasing_the_core_clock (end)
% section overclocking_the_gpu (end)


\section{Final Speedup} % (fold)
\label{sec:final_speedup}
The previous sections showed the how the algorithm compares to the \gls{CPU} over
different work-sizes. Disregarding the ramp-up phase a single \gls{GPU} achieved
a speedup of about 160$\times$. Computing with two \glspl{GPU} the resulting 
speedup was of about 320$\times$. This showed that a suitable algorithm can 
achieve reasonable speedups compared to traditional \glspl{CPU}.

Additionally the scalability over the work-sizes, an additional \glspl{GPU} and
frequency were analyzed and depicted. The result is clear, the algorithm scales
linearly over all three before mentioned parameters. 

\fpAdd{\timegold}{0,0}{1082570,14}
\fpAdd{\timecuda}{0,0}{5465,28}
\fpDiv{\speedup}{\timegold}{\timecuda}

The speedup for an image of 2688$\times$2688 pixel segmented by one
overclocked \gls{GPU} is 
\begin{equation}\label{eq:final_speedup_single}
 S_{single}(n) = \frac{T_{CPU}}{T_{GPU}} = \frac{\unit[\timegold]{ms}}{\unit[\timecuda]{ms}} = \speedup
\end{equation}

\fpAdd{\timecuda}{0,0}{2736,14}
\fpDiv{\speedup}{\timegold}{\timecuda}

and the final speedup by two overclocked \glspl{GPU} is
\begin{equation}\label{eq:final_speedup_multi}
 S_{multi}(n) = \frac{T_{CPU}}{T_{GPU}} = \frac{\unit[\timegold]{ms}}{\unit[\timecuda]{ms}} = \speedup
\end{equation}

That's an impressive number about 400$\times$ faster than the \gls{CPU} version. 
Of course on has to take into account that the used algorithm is highly suitable
for the \gls{GPU}. Nevertheless this chapter showed to which extent algorithms 
can be accelerated on the \gls{GPU}.
% section final_speedup (end)
% chapter performance_and_scalability_ (end)



\chapter{FONT STUFF}

\newlength{\abcd} % for ab..z string length calculation
\settowidth{\abcd}{abcdefghijklmnopqrstuvwxyz} 	
\the\abcd

\newpage
\begin{figure}[ht]
  \centering
	
	\pgfplotsset{every axis x label/.style={at={(1,0)}, above}}
 	\pgfplotsset{every axis y label/.style={at={(0,1)}, left}}

	\pgfplotstableread{Plots/single.gpu.data}\tableA
	\pgfplotstableread{Plots/multi.gpu.data}\tableB	

  \begin{tikzpicture}
		\begin{axis}[
			area style,
     	width=\textwidth,
      height=5cm,
%			ytick={0,0.5,1,1.5},
      xtick={128,384,...,2688},
      axis x line=bottom,
      axis y line=left,
%      xmin=0, xmax=2796, 
      ymin=0, ymax=350,
      xlabel=$x$,
      ylabel=$y$,
		%	enlargelimits=0.01,
    %  ymajorgrids 
			]
     
			\addplot[color=pl1,fill=pf1] table[x=PIX,y=CPU_Speedup] from \tableB;
 			
\addplot[color=pl0,fill=pf0] table[x=PIX,y=Speedup] from \tableA;
 			
			\addlegendentry{\slcsmallcaps{plot1}}
			\addlegendentry{\slcsmallcaps{plot0}}

    \end{axis}

  \end{tikzpicture}%
	\caption{{\ssmallcaps{Gpu}} speedups compared to the {\slcsmallcaps{CPU}} 
					 run-times over image size}%
	\label{fig:multi_gpu_cpu_speedup}
\end{figure}

\begin{figure}[ht]
  \centering
	
	\pgfplotsset{every axis x label/.style={at={(1,0)}, above}}
 	\pgfplotsset{every axis y label/.style={at={(0,1)}, left}}

	\pgfplotstableread{Plots/single.gpu.data}\tableA
	\pgfplotstableread{Plots/multi.gpu.data}\tableB	

  \begin{tikzpicture}
		\begin{axis}[
			area style,
     	width=\textwidth,
      height=5cm,
%			ytick={0,0.5,1,1.5},
      xtick={128,384,...,2688},
      axis x line=bottom,
      axis y line=left,
%      xmin=0, xmax=2796, 
      ymin=0, ymax=350,
      xlabel=$x$,
      ylabel=$y$,
		%	enlargelimits=0.01,
    %  ymajorgrids 
			]
     
			\addplot[color=plot3!80!black,fill=plot3!40!white] table[x=PIX,y=CPU_Speedup] from \tableB;
 	
		\addplot[color=pl2,fill=pf2] table[x=PIX,y=Speedup] from \tableA;
 	
		\addlegendentry{\slcsmallcaps{plot3}}
		\addlegendentry{\slcsmallcaps{plot2}}

    \end{axis}

  \end{tikzpicture}%
	\caption{{\ssmallcaps{Gpu}} speedups compared to the {\slcsmallcaps{CPU}} 
					 run-times over image size}%
	\label{fig:multi_gpu_cpu_speedup}
\end{figure}
\begin{figure}[ht]
  \centering
	
	\pgfplotsset{every axis x label/.style={at={(1,0)}, above}}
 	\pgfplotsset{every axis y label/.style={at={(0,1)}, left}}

	\pgfplotstableread{Plots/single.gpu.data}\tableA
	\pgfplotstableread{Plots/multi.gpu.data}\tableB	

  \begin{tikzpicture}
		\begin{axis}[
			area style,
     	width=\textwidth,
      height=5cm,
%			ytick={0,0.5,1,1.5},
      xtick={128,384,...,2688},
      axis x line=bottom,
      axis y line=left,
%      xmin=0, xmax=2796, 
      ymin=0, ymax=350,
      xlabel=$x$,
      ylabel=$y$,
		%	enlargelimits=0.01,
    %  ymajorgrids 
			]
  
					\addplot[color=plot5!80!black,fill=plot5!40!white] table[x=PIX,y=CPU_Speedup] from \tableB;   
 			\addplot[color=plot4!80!black,fill=plot4!40!white] table[x=PIX,y=Speedup] from \tableA;

			\addlegendentry{\slcsmallcaps{plot5}}
			\addlegendentry{\slcsmallcaps{plot4}}

    \end{axis}

  \end{tikzpicture}%
	\caption{{\ssmallcaps{Gpu}} speedups compared to the {\slcsmallcaps{CPU}} 
					 run-times over image size}%
	\label{fig:multi_gpu_cpu_speedup}
\end{figure}
\begin{figure}[ht]
  \centering
	
	\pgfplotsset{every axis x label/.style={at={(1,0)}, above}}
 	\pgfplotsset{every axis y label/.style={at={(0,1)}, left}}

	\pgfplotstableread{Plots/single.gpu.data}\tableA
	\pgfplotstableread{Plots/multi.gpu.data}\tableB	

  \begin{tikzpicture}
		\begin{axis}[
			area style,
     	width=\textwidth,
      height=5cm,
%			ytick={0,0.5,1,1.5},
      xtick={128,384,...,2688},
      axis x line=bottom,
      axis y line=left,
%      xmin=0, xmax=2796, 
      ymin=0, ymax=350,
      xlabel=$x$,
      ylabel=$y$,
		%	enlargelimits=0.01,
    %  ymajorgrids 
			]
     
					\addplot[color=plot7!80!black,fill=plot7!40!white] table[x=PIX,y=CPU_Speedup] from \tableB;
 	
		\addplot[color=plot6!80!black,fill=plot6!40!white] table[x=PIX,y=Speedup] from \tableA;
 	
		\addlegendentry{\slcsmallcaps{plot7}}
		\addlegendentry{\slcsmallcaps{plot6}}

    \end{axis}

  \end{tikzpicture}%
	\caption{{\ssmallcaps{Gpu}} speedups compared to the {\slcsmallcaps{CPU}} 
					 run-times over image size}%
	\label{fig:multi_gpu_cpu_speedup}
\end{figure}
\begin{figure}[ht]
  \centering
	
	\pgfplotsset{every axis x label/.style={at={(1,0)}, above}}
 	\pgfplotsset{every axis y label/.style={at={(0,1)}, left}}

	\pgfplotstableread{Plots/single.gpu.data}\tableA
	\pgfplotstableread{Plots/multi.gpu.data}\tableB	

  \begin{tikzpicture}
		\begin{axis}[
			area style,
     	width=\textwidth,
      height=5cm,
%			ytick={0,0.5,1,1.5},
      xtick={128,384,...,2688},
      axis x line=bottom,
      axis y line=left,
%      xmin=0, xmax=2796, 
      ymin=0, ymax=350,
      xlabel=$x$,
      ylabel=$y$,
		%	enlargelimits=0.01,
    %  ymajorgrids 
			]
     
					\addplot[color=plot8!80!black,fill=plot8!40!white] table[x=PIX,y=CPU_Speedup] from \tableB;
	
		\addplot[color=plot8!80!black,fill=plot8!40!white] table[x=PIX,y=Speedup] from \tableA;
 	
		\addlegendentry{\slcsmallcaps{plot8}}
		\addlegendentry{\slcsmallcaps{plot8}}

    \end{axis}

  \end{tikzpicture}%
	\caption{{\ssmallcaps{Gpu}} speedups compared to the {\slcsmallcaps{CPU}} 
					 run-times over image size}%
	\label{fig:multi_gpu_cpu_speedup}
\end{figure}
% section multiple_gpus (end)