\chapter{Performance {\itshape{\&}} Scalability} % (fold)
\label{cha:performance_and_scalability_}

To evaluate the performance and the scalability of the implemented mean shift
algorithm several benchmarks will be made. Firstly the algorithm is tested 
against different image sizes. After that a multiple \gls{GPU} version is 
evaluated and tested with the same image sizes as in the previous test. Lastly
the hardware parameters of the \gls{GPU} like the core clock and memory clock
are manipulated to get a clue if the algorithm is memory or computational bound. 
In the case that the algorithm is memory bound one should try to decrease the
communication and if the algorithm is computational bound a restructuring of 
the algorithm could help here. All in all its interesting to see how an algorithm
behaves at different circumstances. In general all runs were performed 20 times
in a row and the mean was taken as the final result.

\section{Varying the Image Size} % (fold)
\label{sec:varying_the_image_size}
The first benchmark varies the image size from 128 $\times$ 128 pixels to 2688
$\times$ 2688 pixels. The image side length is incremented by 128 pixels. The
\autoref{fig:gpu_speedup} shows the \gls{GPU} runtime and the speedup compared
to the \gls{CPU} run time. The \gls{CPU} run time was not included in
\autoref{fig:gpu_speedup} because the range of \gls{CPU} values is 100$\times$ 
larger and the \gls{GPU} values would not be visible. 
\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
		\pgfplotsset{
			every axis legend/.append style={at={(0.02,0.98)}, anchor=north west}
		}
	
    \begin{axis}[
			% colormap/violet,
			% legend columns=2,
      % smooth,
      % stack plots=y,
      % area style,
      ybar, 
			bar width=4pt,
      width=0.88\textwidth,
      height=5cm,
      xtick={128,384,...,2688},
      axis x line=bottom,
      axis y line=left,
      xmin=0, xmax=2796, 
      %ymin=0, ymax=1100,
      xlabel=Image size, ylabel={Milliseconds [ms]}, 
			enlargelimits=0.03,
      ymajorgrids ]
  %    \addplot%[color=plotcolor0!50!black,fill=plotcolor0]
  %    table[x=PIX,y={CPU [ms]}] {Plots/cpu_gpu_runtime.data};%
	%		\addlegendentry{CPU}
  %    \closedcycle;

      \addplot%[color=plotcolor1!50!black,fill=plotcolor1]
      table[x=PIX,y={GPU [ms]}] {Plots/cpu_gpu_runtime.data};%
			\addlegendentry{\slcsmallcaps{GPU}}
%      \closedcycle;

    \end{axis}

		\begin{axis}[
     	width=0.88\textwidth,
      height=5cm,
      xtick={128,384,...,2688},
      axis x line=none,
      axis y line=right,
      xmin=0, xmax=2796, 
      ymin=100, ymax=180,
  %    xlabel=Image size, 
			ylabel={Speedup}, 
			enlargelimits=0.03,
      ymajorgrids ]
      \addplot%[color=plotcolor0!50!black,fill=plotcolor0]
      table[x=PIX,y={Speedup}] {Plots/cpu_gpu_runtime.data};%
%			\addlegendentry{CPU}
%      \closedcycle;
    \end{axis}

  \end{tikzpicture}%
  \caption{GPU runtime and speedup dependent on the image size}%
	\label{fig:gpu_speedup}%
\end{figure}
Its interesting to see that the speedup is increasing faster at the beginning
and than to level off at about 160. This is no surprise because every software
respectively hardware has a ramp-up phase. This ramp-up phase, where no
computation is performed, often involves allocation and initialization of data
and hardware and additionally on this platform the movement of data from the
host to the \gls{GPU} buffers. With smaller image sizes the time for the ramp-up
phase is a significant amount of the complete run-time. With bigger and bigger
image sizes the amount of the ramp-up phase to the complete run-time becomes 
smaller and hence the speedup increases. 

\subsubsection{Linearity} % (fold)
\label{ssub:linearity}
Another fact to consider is how the algorithm is scaling with increasing image
sizes. In \autoref{ch:algorithm_analysis} it was shown that the algorithm
exposes linear complexity $O(n)$ and it is interesting to see how the
implemented algorithm behaves in terms of linearity. An algorithm is linear when
the two metrics for measuring the performance here the image size and the
run-time are proportional. The image size is proportional to the run-time if 
the ratio of the two variables is constant.
\begin{figure}[ht]
  \centering
	
%	\pgfplotsset{every axis x label/.style={at={(1,0)}, above}}
%  \pgfplotsset{every axis y label/.style={at={(0,1)}, left}}

	\pgfplotstableread{Plots/cpu.gpu.linearity.data}\tableA

  \begin{tikzpicture}
		\begin{axis}[
     	width=0.88\textwidth,
      height=5cm,
			ytick={0,0.5,1,1.5},
      xtick={128,384,...,2688},
      axis x line=bottom,
      axis y line=left,
%      xmin=0, xmax=2796, 
      ymin=0, ymax=2,
      xlabel=Image Size,
      ylabel=Normalized Size/Time Ratio,
			enlargelimits=0.01,
    %  ymajorgrids 
			]
     
 			\addplot table[x=PIX,y=GPU] from \tableA;
 			\addplot table[x=PIX,y=CPU] from \tableA;

			\addlegendentry{\slcsmallcaps{GPU}}
			\addlegendentry{\slcsmallcaps{CPU}}

    \end{axis}

  \end{tikzpicture}%
	\caption{Normalized size/time ratios}%
	\label{fig:linearity}
 \end{figure}

The \autoref{fig:linearity} shows the normalized size per time ratios. As it can
be seen the ratio for the \gls{CPU} is constant over image size and the \gls{GPU}
ratio is increasing similar to the speedup because of the before mentioned 
ramp-up phase until it reaches 1 as in the case of the \gls{CPU} ratio. If the
image size is doubled the run-time is doubled. The algorithm scales linearly. 
% subsubsection linearity (end)
% section varying_the_image_size (end)




\section{Multiple gpus} % (fold)
\label{sec:multiple_gpus}

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
	
		\pgfplotsset{
			every axis legend/.append style={at={(0.02,0.98)}, anchor=north west}
		}
		\pgfplotstableread{multi.gpu.data}\tableA
		
    \begin{axis}[
			% colormap/violet,
			% legend columns=2,
      % smooth,
      % stack plots=y,
      % area style,
      ybar, 
			bar width=4pt,
      width=0.88\textwidth,
      height=5cm,
      xtick={128,384,...,2688},
      axis x line=bottom,
      axis y line=left,
      xmin=0, xmax=2796, 
      %ymin=0, ymax=1100,
      %xlabel=Image size, 
			%ylabel={Milliseconds [ms]}, 
			enlargelimits=0.03,
      ymajorgrids 
			]

      \addplot table[x=PIX,y={1 GPU(s) [ms]}] from \tableA;
			\addplot table[x=PIX,y={2 GPU(s) [ms]}] from \tableA;
			
			\addlegendentry{\slcsmallcaps{GPU}}

    \end{axis}

		\begin{axis}[
     	width=0.88\textwidth,
      height=5cm,
      xtick={128,384,...,2688},
      axis x line=none,
      axis y line=right,
      xmin=0, xmax=2796, 
      ymin=100, ymax=180,
  %    xlabel=Image size, 
			ylabel={Speedup}, 
			enlargelimits=0.03,
      ymajorgrids ]
      \addplot%[color=plotcolor0!50!black,fill=plotcolor0]
      table[x=PIX,y={Speedup}] from \tableA;
%			\addlegendentry{CPU}
%      \closedcycle;
    \end{axis}

  \end{tikzpicture}%
  \caption{Multiple GPU runtime and speedup dependent on the image size}%
	\label{fig:multi_gpu_speedup}%
\end{figure}

% section multiple_gpus (end)

\section{Overclocking the GPU} % (fold)
\label{sec:overclocking_the_gpu}
The \gls{GPU} has three \glspl{clock} that can be over-clocked to achieve higher
performance. The first is the core \glspl{clock}, the second \glspl{clock} is
the memory clock and the third clock is the \gls{shader} \gls{clock} where the
shader clock typically moves synchronically with the core as they work on a set
ratio. For example the used \emph{GeForce 8800 GTS 512} features a 650 \gls{MHz}
core, a 1620 \gls{MHz} shader and a 970 \gls{MHz} memory clock. This means this
particular graphics card uses a core to shader \gls{clock} multiplier of
2.5$\times$. Increasing the core \gls{clock} means increasing the shader clock
at the same time. Therefore only the core and memory clock will be considered
for the experiments.

The reason why someone would over-clock a \gls{GPU} is at first hand to increase
the performance and on the second hand to find out if an algorithm is
computational or memory bound by experiment.

Therefore several experiments will be undertaken. One experiment will involve
the change of the core \gls{clock}, second the change of the memory \gls{clock}
and lastly the combination of both \glspl{clock}. All experiments were done with
a free tool, \emph{NVClock}
\footnote{http://www.linuxhardware.org/nvclock/}. NVClock is a small utility
that allows to over-clock {\slcsmallcaps{NVIDIA}} based \glspl{GPU} and adjust the fan speed. This
is important since the \gls{GPU} will run out of specification and could overheat.
The experiments will begin with changing the core clock and evaluation of the
results. 

\subsection{Increasing the Core Clock} % (fold)
\label{sub:increasing_the_core_clock}
For the first experiment the core clock will be changed by steps of 10 \gls{MHz}
and as a dependency the shader clock as well. As a starting point the default
648 \gls{MHz} of the core clock will be used and the memory clock will be fixed
at the default of 972 \gls{MHz}. Experiments have shown that the maximal
achievable core clock is 783 \gls{MHz}. All clocks were read out from hardware
with \emph{NVClock}. Additionally it must be noted that all frequency settings
were not accepted one-on-one which means an increase per software of 10 \gls{MHz}
doesn't mean an increase of 10 \gls{MHz} in hardware due to hardware restrictions.
The \autoref{tab:core_sw_hw} shows the frequency set in software and the resulting
real frequency in hardware. 
\begin{table}[ht]
    \centering
	\pgfkeys{/pgf/number format/.cd,fixed,precision=0}
	
	\pgfplotstabletypeset%[display columns/0/.style={select equal part entry of={0}{2},string type}, 
												%display columns/1/.style={select equal part entry of={0}{2},string type}, 
												%display columns/2/.style={select equal part entry of={1}{2},string type}, 
												%display columns/3/.style={select equal part entry of={1}{2},string type}, 
												%columns={{Software [MHz]},{Hardware [MHz]},{Software [MHz]},{Hardware [MHz]}} ]
												{Tables/core.sw.hw.data}

 	\caption{Frequencies set in software and the resulting hardware frequencies}
 	\label{tab:core_sw_hw}
\end{table}


\paragraph{Image Size 1024$\times$1024 pixel} % (fold)
\label{par:image_size_256_times_256_pixel}

% paragraph image_size_256_times_256_pixel (end)


% subsection increasing_the_core_clock (end)











% section overclocking_the_gpu (end)


% chapter performance_and_scalability_ (end)
