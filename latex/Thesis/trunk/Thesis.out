\BOOKMARK [1][-]{acknowledgments.1}{Acknowledgments}{}% 1
\BOOKMARK [0][]{chapter.1}{1 Introduction}{}% 2
\BOOKMARK [1][-]{section.1.1}{1.1 Aims \046 Objectives}{chapter.1}% 3
\BOOKMARK [1][-]{section.1.2}{1.2 Initial Time plan}{chapter.1}% 4
\BOOKMARK [0][]{chapter.2}{2 Literature Review}{}% 5
\BOOKMARK [1][-]{section.2.1}{2.1 General Purpose Computing on Graphics Hardware}{chapter.2}% 6
\BOOKMARK [1][-]{section.2.2}{2.2 Parallel Programming \046 Thinking}{chapter.2}% 7
\BOOKMARK [1][-]{section.2.3}{2.3 Embarrassingly Parallel Algorithms}{chapter.2}% 8
\BOOKMARK [2][-]{subsection.2.3.1}{2.3.1 Computations which Map Well to GPUs}{section.2.3}% 9
\BOOKMARK [2][-]{subsection.2.3.2}{2.3.2 Ray Tracing}{section.2.3}% 10
\BOOKMARK [2][-]{subsection.2.3.3}{2.3.3 Photon Mapping}{section.2.3}% 11
\BOOKMARK [2][-]{subsection.2.3.4}{2.3.4 Multiple Precision Arithmetic}{section.2.3}% 12
\BOOKMARK [2][-]{subsection.2.3.5}{2.3.5 High Dynamic Range}{section.2.3}% 13
\BOOKMARK [2][-]{subsection.2.3.6}{2.3.6 Genetic Algorithms}{section.2.3}% 14
\BOOKMARK [2][-]{subsection.2.3.7}{2.3.7 Chaos Theory}{section.2.3}% 15
\BOOKMARK [2][-]{subsection.2.3.8}{2.3.8 Image Processing}{section.2.3}% 16
\BOOKMARK [1][-]{section.2.4}{2.4 Summary \046 Conclusion}{chapter.2}% 17
\BOOKMARK [0][]{chapter.3}{3 Parallel Processing with GPUs}{}% 18
\BOOKMARK [1][-]{section.3.1}{3.1 Parallel Architectures}{chapter.3}% 19
\BOOKMARK [1][-]{section.3.2}{3.2 The Tesla Architecture}{chapter.3}% 20
\BOOKMARK [1][-]{section.3.3}{3.3 Common Unified Device Architecture \(CUDA\)}{chapter.3}% 21
\BOOKMARK [1][-]{section.3.4}{3.4 CUDA Programming Model}{chapter.3}% 22
\BOOKMARK [1][-]{section.3.5}{3.5 A Simple Example}{chapter.3}% 23
\BOOKMARK [1][-]{section.3.6}{3.6 Porting Strategy for GPUs}{chapter.3}% 24
\BOOKMARK [0][]{chapter.4}{4 Feasibility Study}{}% 25
\BOOKMARK [1][-]{section.4.1}{4.1 An Overview of Ray Tracing}{chapter.4}% 26
\BOOKMARK [1][-]{section.4.2}{4.2 Ray Casting}{chapter.4}% 27
\BOOKMARK [1][-]{section.4.3}{4.3 Performance tuning}{chapter.4}% 28
\BOOKMARK [1][-]{section.4.4}{4.4 Demystifing the Myths}{chapter.4}% 29
\BOOKMARK [0][]{chapter.5}{5 Mean Shift}{}% 30
\BOOKMARK [1][-]{section.5.1}{5.1 Density Estimation}{chapter.5}% 31
\BOOKMARK [1][-]{section.5.2}{5.2 Kernel Density Estimation}{chapter.5}% 32
\BOOKMARK [1][-]{section.5.3}{5.3 Kernel and their Properties}{chapter.5}% 33
\BOOKMARK [1][-]{section.5.4}{5.4 Mean Shift}{chapter.5}% 34
\BOOKMARK [2][-]{subsection.5.4.1}{5.4.1 Density Gradient Estimation}{section.5.4}% 35
\BOOKMARK [2][-]{subsection.5.4.2}{5.4.2 Mean Shift Method}{section.5.4}% 36
\BOOKMARK [1][-]{section.5.5}{5.5 Filtering \046 Segmentation}{chapter.5}% 37
\BOOKMARK [2][-]{subsection.5.5.1}{5.5.1 Mean Shift Filtering}{section.5.5}% 38
\BOOKMARK [2][-]{subsection.5.5.2}{5.5.2 Mean Shift Segmentation}{section.5.5}% 39
\BOOKMARK [0][]{chapter.6}{6 Mean Shift Algorithm Analysis}{}% 40
\BOOKMARK [1][-]{section.6.1}{6.1 Profiling the Original Code}{chapter.6}% 41
\BOOKMARK [1][-]{section.6.2}{6.2 Amdahl's law}{chapter.6}% 42
\BOOKMARK [1][-]{section.6.3}{6.3 Data \046 Task Parallelism}{chapter.6}% 43
\BOOKMARK [2][-]{subsection.6.3.1}{6.3.1 Task Parallelism}{section.6.3}% 44
\BOOKMARK [2][-]{subsection.6.3.2}{6.3.2 Data Parallelism}{section.6.3}% 45
\BOOKMARK [1][-]{section.6.4}{6.4 Data Flow}{chapter.6}% 46
\BOOKMARK [1][-]{section.6.5}{6.5 Summary}{chapter.6}% 47
\BOOKMARK [0][]{chapter.7}{7 Mean Shift Algorithm Design}{}% 48
\BOOKMARK [1][-]{section.7.1}{7.1 Programming Model}{chapter.7}% 49
\BOOKMARK [1][-]{section.7.2}{7.2 Thread Batching}{chapter.7}% 50
\BOOKMARK [1][-]{section.7.3}{7.3 Device Memory space}{chapter.7}% 51
\BOOKMARK [1][-]{section.7.4}{7.4 Warps}{chapter.7}% 52
\BOOKMARK [1][-]{section.7.5}{7.5 Program Flow}{chapter.7}% 53
\BOOKMARK [1][-]{section.7.6}{7.6 Summary}{chapter.7}% 54
\BOOKMARK [0][]{chapter.8}{8 Implementation}{}% 55
\BOOKMARK [1][-]{section.8.1}{8.1 Reference Implementation}{chapter.8}% 56
\BOOKMARK [1][-]{section.8.2}{8.2 The Host Part}{chapter.8}% 57
\BOOKMARK [2][-]{section*.15}{Progress \046 Result Verification}{section.8.2}% 58
\BOOKMARK [1][-]{section.8.3}{8.3 The Device Part}{chapter.8}% 59
\BOOKMARK [0][]{chapter.9}{9 Optimization Strategies}{}% 60
\BOOKMARK [1][-]{section.9.1}{9.1 Test \046 Benchmark Configuration}{chapter.9}% 61
\BOOKMARK [1][-]{section.9.2}{9.2 Offload Compute Intensive Parts}{chapter.9}% 62
\BOOKMARK [1][-]{section.9.3}{9.3 Global Memory \046 Coalescing}{chapter.9}% 63
\BOOKMARK [1][-]{section.9.4}{9.4 Division Instruction Optimization}{chapter.9}% 64
\BOOKMARK [1][-]{section.9.5}{9.5 Execution Configurations}{chapter.9}% 65
\BOOKMARK [1][-]{section.9.6}{9.6 Native Data Types}{chapter.9}% 66
\BOOKMARK [1][-]{section.9.7}{9.7 Avoid Branch Divergence}{chapter.9}% 67
\BOOKMARK [1][-]{section.9.8}{9.8 Shared Memory}{chapter.9}% 68
\BOOKMARK [1][-]{section.9.9}{9.9 Know the Algorithm}{chapter.9}% 69
\BOOKMARK [2][-]{section*.17}{Attractor}{section.9.9}% 70
\BOOKMARK [1][-]{section.9.10}{9.10 Unrolling Loops and Multiplications}{chapter.9}% 71
\BOOKMARK [1][-]{section.9.11}{9.11 Summary}{chapter.9}% 72
\BOOKMARK [0][]{chapter.10}{10 Performance \046 Scalability}{}% 73
\BOOKMARK [1][-]{section.10.1}{10.1 Varying the Image Size}{chapter.10}% 74
\BOOKMARK [2][-]{subsection.10.1.1}{10.1.1 Linearity}{section.10.1}% 75
\BOOKMARK [1][-]{section.10.2}{10.2 Multiple gpus}{chapter.10}% 76
\BOOKMARK [1][-]{section.10.3}{10.3 Overclocking the gpu}{chapter.10}% 77
\BOOKMARK [2][-]{subsection.10.3.1}{10.3.1 Increasing the Memory Clock}{section.10.3}% 78
\BOOKMARK [2][-]{subsection.10.3.2}{10.3.2 Increasing the Core Clock}{section.10.3}% 79
\BOOKMARK [3][-]{figure.caption.27}{Multiple gpus overclocked}{subsection.10.3.2}% 80
\BOOKMARK [1][-]{section.10.4}{10.4 Final Speedup}{chapter.10}% 81
\BOOKMARK [0][]{chapter.11}{11 Conclusions \046 Further Work}{}% 82
\BOOKMARK [1][-]{section.11.1}{11.1 Further Work}{chapter.11}% 83
\BOOKMARK [2][-]{section*.29}{Fermi}{section.11.1}% 84
\BOOKMARK [0][]{appendix.A}{A Reference Tables}{}% 85
\BOOKMARK [0][]{appendix.B}{B Host \046 Kernel Source Code }{}% 86
